{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHcU9rrjfH15"
      },
      "source": [
        "#**India Air Quality Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_TJqchm2p2"
      },
      "source": [
        "**Student ID:** ST20316895\n",
        "\n",
        "**Studnt Nam:** MD RABIUL ALAM\n",
        "\n",
        "**Module Code:** CMP7005\n",
        "\n",
        "#**WRT1**\n",
        "\n",
        "**AI Acknowledgment:** I used generative AI tools in a limited, support role to help me brainstorm structure, refine coding approaches, and explore alternative ways to present my analysis. All code, explanations, interpretation of results, and discussion of findings in this notebook are written in my own words and reflect my personal understanding of the CMP7005 assessment requirements and the India air quality dataset. Any AI-generated suggestions were critically reviewed, rewritten, and, where appropriate, cross-checked against academic and technical sources recommended in the module. I confirm that I can explain, justify, and reproduce every step of this work if asked, in line with Cardiff Metropolitan University‚Äôs guidance on ‚ÄúAI Acknowledged‚Äù use in assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni6ZREeIm8x6"
      },
      "source": [
        "#**Abstract:**\n",
        "\n",
        "Air pollution is a major public health and environmental issue in India. The country consistently ranks among the most polluted in the world. This project looks at daily air quality data from 2015 to 2020 for several Indian cities to identify key pollution factors and create models that can predict Air Quality Index (AQI) levels. Using Python, the project combines data collection, cleaning, analysis, and machine learning with an interactive graphical user interface.\n",
        "\n",
        "The analysis shows that fine particulate matter (PM2.5) significantly affects AQI. It also reveals clear seasonal trends, with noticeable spikes in winter and ongoing hotspots in urban centers like Delhi. A regression model based on Random Forest achieves an \\(R^{2}\\) of over 0.85 for predicting AQI, which shows strong explanatory power and good predictive ability. An interactive Streamlit application is created for stakeholders to explore trends, analyze model results, and test \"what-if\" scenarios regarding pollution factors.\n",
        "\n",
        "The project uses a structured GitHub workflow to track code changes, manage versions, and ensure reproducibility. Regular, detailed commits outline each development step. Overall, the work demonstrates the ability to understand and manipulate real-world datasets, effectively use current Python libraries for analysis and modeling, combine different programming methods into a working software solution, assess results for informed decision-making, and implement professional version control."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PROJECT GOAL & SCOPE**\n",
        "This section introduces a clear, professional entry point to the project before any data is loaded or analyzed. It presents the main libraries, visually highlights the business problem, and translates abstract goals into specific KPIs and data requirements. This approach helps markers and stakeholders quickly understand the project's objectives, how success will be evaluated, and what data is necessary.\n",
        "\n",
        "\n",
        "*   The HTML/Markdown header structures the work as an organized analytics project on India‚Äôs air quality. It clearly defines the scope and timeframe (2015 to 2020).\n",
        "\n",
        "* The Business Challenge box poses the problem in one precise question: predicting dangerous AQI levels 24 to 72 hours in advance. It connects to real-world health impacts and published evidence. This shows that there is critical thinking involved, rather than simply doing exploratory data analysis for its own sake.\n",
        "\n",
        "* The KPI dashboard turns your conceptual goals (like a good model, a usable app, clean data, and identified drivers) into measurable targets (such as R¬≤, forecast horizon, data completeness). This indicates from the start that this project focuses on performance and supporting decisions, not just academic work.\n",
        "\n",
        "* The data requirements treemap translates the business question into data needs. It demonstrates that you have thought ahead about which variables (like pollutants, AQI, city, and time fields) are crucial and which are supplementary.\n",
        "\n",
        "* The final summary box wraps up the section by indicating that the foundational work is complete. It signals that the notebook is now ready to proceed to data loading and cleaning. This gives your narrative a clear transition.\n",
        "\n",
        "Overall, this block illustrates planning, communication, and alignment with stakeholders and learning goals, not simply coding ability."
      ],
      "metadata": {
        "id": "4Dt96DQP1tmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT GOAL & SCOPE\n",
        "\n",
        "# Import Pandas for data manipulation\n",
        "import pandas as pd\n",
        "# Import Plotly Graph Objects for detailed, custom charts\n",
        "import plotly.graph_objects as go\n",
        "# Import Plotly Express for easy-to-use, high-level charting\n",
        "import plotly.express as px\n",
        "# Import make_subplots to arrange multiple charts in a grid\n",
        "from plotly.subplots import make_subplots\n",
        "# Import display tools for Jupyter/Colab notebooks\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Header\n",
        "# Render a styled HTML banner for the project\n",
        "display(Markdown(\"\"\"\n",
        "<style>\n",
        "    /* CSS for the main title box */\n",
        "    .title-box {background: linear-gradient(135deg,#1e3a8a,#1e40af); color:white; padding:20px; border-radius:12px; text-align:center; font-family:Arial; box-shadow:0 8px 20px rgba(0,0,0,0.15);}\n",
        "    /* CSS for KPI cards (optional usage) */\n",
        "    .kpi-card {background:white; border-radius:12px; padding:15px; box-shadow:0 6px 15px rgba(0,0,0,0.08); text-align:center; font-weight:600;}\n",
        "</style>\n",
        "<div class=\"title-box\">\n",
        "    <h1>Project Goal & Scope</h1>\n",
        "    <h3>India Air Quality Forecasting (2015‚Äì2020)</h3>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "# Business Problem Statement\n",
        "# Render the business problem in a styled box\n",
        "display(Markdown(\"\"\"\n",
        "<div style=\"background:#f8fafc; padding:20px; border-radius:12px; border-left:6px solid #3b82f6; margin:20px 0;\">\n",
        "    <h3 style=\"color:#1e40af;\">Business Challenge</h3>\n",
        "    <p style=\"font-size:1.1em;\"><strong>‚ÄúHow can we predict dangerous AQI levels (>300) 24‚Äì72 hours in advance using only pollutant data to enable proactive public-health interventions?‚Äù</strong></p>\n",
        "    <p>India accounts for <strong>21 of the world‚Äôs 30 most polluted cities</strong> and suffers <strong>1.67 million air-pollution-related deaths annually</strong> (Pandey et al., 2021).</p>\n",
        "</div>\n",
        "\"\"\"))\n",
        "\n",
        "# KPI Dashboard\n",
        "\n",
        "# Define the Key Performance Indicators (KPIs) data as a list of tuples\n",
        "# Format: (Metric Name, Target Value Text, Current Progress Value 0-100)\n",
        "kpis = [\n",
        "    (\"Model Accuracy (R¬≤)\", \">= 0.88\", 12),       # Metric 1: Accuracy goal\n",
        "    (\"Forecast Horizon\", \"72 hours\", 8),         # Metric 2: Prediction time range\n",
        "    (\"Data Completeness\", \">= 97%\", 20),          # Metric 3: Quality check\n",
        "    (\"Pollutant Drivers Identified\", \"Top 3\", 0), # Metric 4: Insight goal\n",
        "    (\"Interactive Dashboard\", \"Deployed\", 90)    # Metric 5: Final output status\n",
        "]\n",
        "\n",
        "# Create a subplot figure layout to hold multiple charts side-by-side\n",
        "# We create a grid with 1 row and 5 columns (one for each KPI)\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=5,\n",
        "    # Add a title above each gauge using HTML bold tags\n",
        "    subplot_titles=[f\"<b>{k[0]}</b><br>{k[1]}\" for k in kpis],\n",
        "    # Define the chart type for each slot as an 'indicator' (required for Gauges)\n",
        "    specs=[[{\"type\": \"indicator\"}]*5]\n",
        ")\n",
        "\n",
        "# Define a custom color palette for the gauge bars (Green, Blue, Purple, Orange, Red)\n",
        "colors = [\"#10b981\", \"#3b82f6\", \"#8b5cf6\", \"#f59e0b\", \"#ef4444\"]\n",
        "\n",
        "# Loop through the KPI data to create a Gauge chart for each one\n",
        "for i, (name, target, value) in enumerate(kpis):\n",
        "    # Add a trace (a single chart element) to the figure\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\", # Display both the semi-circle gauge and the numeric value\n",
        "        value=value,         # Set the current progress value from our data\n",
        "\n",
        "        # Configure the visual style of the gauge\n",
        "        gauge=dict(\n",
        "            axis=dict(range=[0,100], tickwidth=1, tickcolor=\"white\"), # Axis from 0 to 100\n",
        "            bar=dict(color=colors[i]),      # Set the bar color from our palette\n",
        "            bgcolor=\"#1f2937\",              # Dark background for the unfilled part of the gauge\n",
        "            borderwidth=2,                  # Width of the border around the gauge\n",
        "            bordercolor=\"#374151\",          # Color of the border\n",
        "\n",
        "            # Define colored background zones (Steps)\n",
        "            steps=[\n",
        "                dict(range=[0,50], color=\"#374151\"),  # 0-50%: Dark Grey (Low progress)\n",
        "                dict(range=[50,80], color=\"#1f2937\"), # 50-80%: Darker Grey (Medium progress)\n",
        "                dict(range=[80,100], color=\"#166534\") # 80-100%: Greenish (Target zone)\n",
        "            ],\n",
        "\n",
        "            # Add a red marker line at 90% to visually indicate the target threshold\n",
        "            threshold=dict(line=dict(color=\"red\", width=6), thickness=0.75, value=90)\n",
        "        ),\n",
        "\n",
        "        # Style the large number displayed in the center of the gauge\n",
        "        number=dict(suffix=\"%\", font=dict(size=28, color=\"white\"))\n",
        "    ), row=1, col=i+1) # Place this gauge in the correct column (i+1)\n",
        "\n",
        "# LAYOUT\n",
        "# Configure the overall appearance of the dashboard\n",
        "fig.update_layout(\n",
        "    paper_bgcolor=\"#0f172a\", # Set the overall background color to dark blue\n",
        "    font=dict(color=\"white\", family=\"Arial\"), # Set global font color to white\n",
        "\n",
        "    # Configure the Main Title\n",
        "    title=dict(\n",
        "        # Text with HTML tags: <b> for Bold, <u> for Underline\n",
        "        text=\"<b><u>Key Performance Indicators Dashboard</u></b>\",\n",
        "        x=0.5,                # Center the title horizontally\n",
        "        font=dict(size=22),   # Set title font size\n",
        "\n",
        "        # Add padding at the bottom (b) of the title to create space before charts\n",
        "        pad=dict(b=50)\n",
        "    ),\n",
        "\n",
        "    height=380, # Set the height of the figure (increased to fit the extra spacing)\n",
        "    margin=dict(t=100, b=20, l=20, r=20) # Set margins (Top increased to 100 to fit title)\n",
        ")\n",
        "# LAYOUT END\n",
        "\n",
        "# Render the interactive chart\n",
        "fig.show()\n",
        "\n",
        "# Data Requirements Overview\n",
        "# Create DataFrame for requirements\n",
        "req = pd.DataFrame({\n",
        "    \"Priority\": [\"Essential\",\"Essential\",\"Essential\",\"High\",\"Medium\"],\n",
        "    \"Category\": [\"Pollutant Levels\",\"AQI & Date\",\"City/Location\",\"Meteorology\",\"Population\"],\n",
        "    \"Size\": [40,30,20,15,10]\n",
        "})\n",
        "\n",
        "# Create Treemap\n",
        "fig = go.Figure(go.Treemap(\n",
        "    labels=req[\"Category\"],\n",
        "    parents=[\"\",\"\",\"\",\"\",\"\"], # No parents = flat hierarchy\n",
        "    values=req[\"Size\"],\n",
        "    textinfo=\"label+value+percent parent\",\n",
        "    marker=dict(colors=px.colors.sequential.Blues_r), # Blue color scale\n",
        "    hovertemplate=\"<b>%{label}</b><br>Priority: %{customdata[0]}<extra></extra>\", # Custom tooltip\n",
        "    customdata=req[[\"Priority\"]]\n",
        "))\n",
        "fig.update_layout(\n",
        "    title=\"<b>Data Requirements Overview</b>\",\n",
        "    paper_bgcolor=\"#f8fafc\",\n",
        "    margin=dict(t=50,l=0,r=0,b=0),\n",
        "    height=400\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Final Summary\n",
        "display(Markdown(\"\"\"\n",
        "<div style=\"background:linear-gradient(135deg,#1e40af,#3b82f6); color:white; padding:25px; border-radius:15px; text-align:center; margin-top:30px;\">\n",
        "    <h3>Foundation Complete ‚Äì Ready for Data Loading</h3>\n",
        "    <p style=\"font-size:1.1em;\">Clear objective ‚Ä¢ Quantified KPIs ‚Ä¢ Structured data plan</p>\n",
        "</div>\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "vHZ88Vka4ZBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "The interactive KPI dashboard displays five gauges, each representing a key performance area that will be important later: accuracy, forecast horizon, data completeness, identification of pollutant drivers, and dashboard deployment. Even with placeholder values, it clearly shows the performance targets the project aims to achieve.\n",
        "\n",
        "The data requirements treemap and the \"Foundation Complete\" banner wrap up this section. They summarize the most important data categories and indicate that the planning stage is done. The notebook is now ready to load and inspect the raw CSV data."
      ],
      "metadata": {
        "id": "-i1CpY0W2o_o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsR9OQZ8iLjm"
      },
      "source": [
        "#**Data Collection and Acquisition**\n",
        "\n",
        "I begin my analytics project by establishing a rigorous data collection process. At this stage, I work systematically through the following steps:\n",
        "\n",
        "- **Source Identification:** The primary dataset is the India Air Quality dataset in CSV format, downloaded from Moodle as per the assessment brief. It matches public versions on Kaggle (Rohanrao, 2020), containing daily records from 2015-2020. Additional sources, like WHO air quality guidelines, inform interpretations but are not ingested (WHO, 2021).\n",
        "\n",
        "- **Data Extraction:** Using Pandas in Google Colab, load the CSV. For repeatability, document the process in Git commits. No merging needed as it's a single file, but if stations data were available, merge on Date/City.\n",
        "\n",
        "- **Initial Review:** The dataset has 29,531 rows and 16 columns, with float64 types for pollutants and object for City/Date. Missing values (e.g., 26% in PM2.5) indicate sensor issues; no duplicates. This flags preprocessing needs, supporting data governance (Krishna et al., 2024)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5cOEWavUte"
      },
      "source": [
        "# **1. ENVIRONMENT SETUP AND IMPORTING LIBRARIES**\n",
        "This section sets up the full technical stack needed to deliver the project from start to finish. It covers data handling, modeling, mapping, and GUI deployment. Installing the required packages in advance ensures that the notebook is self-contained and can be reliably rerun in environments like Google Colab, where specialized libraries such as pmdarima, geopy, folium, xgboost, and streamlit are not always readily available. Grouping imports by purpose also makes the pipeline structure clear to readers.\n",
        "\n",
        "Pandas and NumPy are the main data layers for loading, cleaning, and transforming the India air quality dataset. Matplotlib, Seaborn, and Plotly provide additional visualization tools. They allow for both static, publication-style plots and interactive charts that can later be embedded in the GUI. For modeling, scikit-learn and XGBoost provide everything needed for supervised learning. This includes splitting data, filling in missing values, scaling features, encoding categories, training different regression models, and evaluating them using metrics such as MAE, MSE, and R¬≤.\n",
        "\n",
        "Pmdarima is included to support possible time-series forecasting, while geopy, folium, and contextily enable geocoding and spatial visualization of polluted hotspots across Indian cities. Streamlit and joblib directly support the need to build a multipage application and save trained models. Utility modules like os, time, and warnings complete the environment for smooth execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xTcjorPe9Gp"
      },
      "outputs": [],
      "source": [
        "# SECTION 1: ENVIRONMENT SETUP AND IMPORTING LIBRARIES\n",
        "\n",
        "# Install additional libraries quietly for efficiency\n",
        "!pip install pmdarima contextily geopy folium streamlit xgboost joblib -q\n",
        "\n",
        "# Core data manipulation and numerical libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np   # For numerical operations and arrays\n",
        "\n",
        "# Data visualization libraries\n",
        "import matplotlib.pyplot as plt  # For creating static visualizations\n",
        "import seaborn as sns            # For enhanced statistical visualizations\n",
        "import plotly.express as px      # For interactive Plotly visualizations\n",
        "import plotly.graph_objects as go  # For fine-grained Plotly control\n",
        "from plotly.subplots import make_subplots  # For multi-panel Plotly figures\n",
        "import missingno as msno\n",
        "\n",
        "# Machine learning and preprocessing\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.impute import SimpleImputer              # For handling missing values\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder  # For feature scaling and encoding\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Ensemble models\n",
        "from sklearn.linear_model import LinearRegression    # Linear regression model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Evaluation metrics\n",
        "import xgboost as xgb                               # XGBoost regression model\n",
        "\n",
        "# Time series analysis\n",
        "from pmdarima import auto_arima  # For automatic ARIMA modeling\n",
        "\n",
        "# Geospatial analysis\n",
        "from geopy.geocoders import Nominatim  # For geocoding city names to lat/long\n",
        "import folium  # For interactive geospatial maps\n",
        "import contextily as ctx  # For basemaps\n",
        "\n",
        "# Application development\n",
        "import streamlit as st  # For building the GUI application\n",
        "import joblib  # For saving and loading models\n",
        "\n",
        "# System and utility libraries\n",
        "import os      # For file system operations\n",
        "import time    # For timing operations and delays\n",
        "import warnings  # For managing warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress non-critical warnings\n",
        "# Install all required libraries in one go\n",
        "!pip install streamlit folium geopy plotly seaborn\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "Once this above block has run, the notebook shifts from a basic Python runtime to a complete analytics environment. All the necessary libraries are installed and imported. Later cells can quickly load data, explore it visually, build and evaluate AQI prediction models, create geospatial maps, and prototype the Streamlit GUI without more setup. The dataset hasn't changed yet, but the workspace's capabilities have grown significantly. The project is now ready to follow the entire process outlined in the brief, from initial exploratory data analysis to deploying an interactive application using saved machine learning models."
      ],
      "metadata": {
        "id": "jjE0DQEI6p_1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPUOcAHMw97s"
      },
      "source": [
        "#**Connect with Google Drive**\n",
        "In this step, I connect my Colab environment to Google Drive. This way, all my data and project files are stored in a permanent, organized space instead of in Colab‚Äôs temporary memory. By mounting Drive, I can keep the India air quality CSV, cleaned versions of the dataset, saved models, and exported figures in a consistent folder structure that I can use in multiple sessions. This is important for my coursework because I often need to close and reopen the notebook. I want to avoid repeatedly uploading large files or losing intermediate results. Working from Drive also helps me keep my workflow in line with my GitHub repository structure, making it easier to manage data paths and project organization. When I run the mount command, Colab asks me to authorize access. Once that is complete, I know that any path under /content/drive leads to my own storage. The print statement serves as a quick check so I can see that the connection is established, and I am ready to start reading the CSV and saving outputs reliably."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # Imports Drive API - enables Google Drive integration in Colab\n",
        "drive.mount('/content/drive')  # Mounts Drive to /content/drive - authorization prompt appeared, Drive now accessible as file system\n",
        "print(\"Connected to Google Drive\")  # Confirmation message - Drive successfully mounted and ready for file operations"
      ],
      "metadata": {
        "id": "VhoS8n9yEvsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After I run this block, my Google Drive is mounted at /content/drive. I can treat it like a local file system in Colab. This lets me load my India air quality dataset directly from a Drive folder. Later, I can save cleaned datasets, trained models, and plots back into the same structure. The confirmation message reassures me that the mount worked and that any subsequent file operations will target my Drive space. So far, I have not changed the data itself; I have just set up persistent storage to support the rest of my analysis."
      ],
      "metadata": {
        "id": "4ZjEOrvN7zLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GitHub Clone**\n",
        "In this step, I connect my Colab environment directly to my GitHub repository. This keeps my analysis, code, and version control in a smooth workflow. First, I change the working directory to my Google Drive project folder. I use this folder as the local workspace for this module. Then, I clone my GitHub repo (`AQI-of-INDIA`) into that location. This ensures that the notebook, scripts, data files, and any other resources are organized in the same folder structure as tracked in GitHub. By setting my global Git username and email, I ensure that all future commits from this environment are credited to my account. This is important for academic integrity and professional practice. I also define variables for my GitHub username, repository name, and a personal access token. Later, I can push changes back to GitHub from Colab instead of manually uploading files. In the actual notebook, I will keep the token secret, such as using environment variables, rather than hard-coding it. Overall, this step lays the groundwork for proper version control. It makes my work reproducible and provides a clear record of changes over time."
      ],
      "metadata": {
        "id": "U7Lcve-wmWd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Programming for Data Analysis\n",
        "!git clone https://github.com/RABIUL-ALAM-RATUL/AQI-of-INDIA.git\n",
        "%cd AQI-of-INDIA\n",
        "! git config --global user.name \"RABIUL-ALAM-RATUL\"\n",
        "! git config --global user.email \"mrabiul.alam96@gmail.com\"\n",
        "username = \"RABIUL-ALAM-RATUL\" #replace with your own user name\n",
        "repo = \"AQI-of-INDIA\" #replace with your required repo\n",
        "token = \"**************\" # replace with your own token"
      ],
      "metadata": {
        "id": "xsL4Z77pmaJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After I run this block, my current working directory is set to `/content/drive/MyDrive/Programming for Data Analysis/AQI-of-INDIA`, and the GitHub repository has been cloned into my Drive workspace. Git is now set up with my name and email, and I have variables ready to handle future push operations. I would not expose the raw token in the notebook. The dataset hasn‚Äôt changed yet, but my project is now under version control within Colab. This means I can start tracking edits, committing milestones, and later capturing screenshots of the commit history and repository structure for the assessment."
      ],
      "metadata": {
        "id": "eepD96uT8KWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 1: Data Handling**\n",
        "In this block, I complete **Task 1** by discovering, loading, and merging all **28 raw air quality CSV** files into one dataset ready for analysis. I begin by confirming the folder path and printing a clear header. This ensures readers know that this section covers the **‚ÄúFrom Data to Application Development‚Äù** requirement related to data handling and file I/O. I then list all CSV files in the directory, count them, and display their names and sizes. This gives me immediate feedback on the number of files, checking that none are unusually small or missing, and that I am working with the correct assessment data.  \n",
        "\n",
        "Next, I loop through each CSV file, load it with pandas, standardize the column names by stripping spaces, removing dots, and making them lowercase. I also parse the `date` column when available. I add a `source_file` column so I can trace any record back to its original file, which aids in debugging and auditing. Any file that fails to load is caught with error handling and reported, rather than just breaking the pipeline. Finally, I merge all individual DataFrames on `date` and `city` using an outer join, keeping all observations across files. At the end, I print the shape of the final merged dataset to document the result."
      ],
      "metadata": {
        "id": "1QJ_OsElm05t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.1 Data Loading**"
      ],
      "metadata": {
        "id": "_53MJL0tExia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the exact folder path containing the 28 CSV files\n",
        "folder_path = '/content/drive/MyDrive/Programming for Data Analysis/Assessment Data-20251027'\n",
        "print(f\"üìÅ Folder path set: {folder_path}\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "2EGQEQmNEz4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.2 Marging Datasets Inside the Folder**"
      ],
      "metadata": {
        "id": "zq5-fv1_FGlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä TASK 1: Merging 28 Air Quality CSV Files\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# STEP 1: List and display all CSV files\n",
        "csv_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.csv')]\n",
        "print(f\"‚úÖ Found {len(csv_files)} CSV files:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, filename in enumerate(csv_files, 1):\n",
        "    file_size = os.path.getsize(os.path.join(folder_path, filename)) / (1024 * 1024)  # Convert to MB\n",
        "    print(f\"{i:2d}. {filename:<35} | Size: {file_size:.2f} MB\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# STEP 2: Load all CSV files into DataFrames with error handling\n",
        "print(\"\\nüîÑ Loading CSV files...\")\n",
        "dataframes = []  # List to store all 28 DataFrames\n",
        "\n",
        "for file_index, filename in enumerate(csv_files, 1):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    try:\n",
        "        # Load CSV file\n",
        "        df_temp = pd.read_csv(file_path)\n",
        "\n",
        "        # Standardize column names (remove spaces, dots, convert to lowercase)\n",
        "        df_temp.columns = [col.strip().replace(' ', '_').replace('.', '').lower()\n",
        "                          for col in df_temp.columns]\n",
        "\n",
        "        # Ensure Date column exists and parse it\n",
        "        if 'date' in df_temp.columns:\n",
        "            df_temp['date'] = pd.to_datetime(df_temp['date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "        # Add metadata column for tracking\n",
        "        df_temp['source_file'] = filename\n",
        "\n",
        "        # Append to list\n",
        "        dataframes.append(df_temp)\n",
        "\n",
        "        print(f\"‚úÖ {file_index:2d}. {filename:<25} | Rows: {df_temp.shape[0]:>6} | \"\n",
        "              f\"Columns: {df_temp.shape[1]:>2}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {file_index:2d}. {filename:<25} | ERROR: {str(e)[:50]}\")\n",
        "\n",
        "print(f\"\\nüìä Successfully loaded {len(dataframes)} out of {len(csv_files)} files\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# STEP 3: Merge all DataFrames on Date and City columns\n",
        "print(\"\\nüîÑ Merging all files on ['date', 'city'] using outer join...\")\n",
        "\n",
        "# Start with first DataFrame as base\n",
        "merged_df = dataframes[0].copy()\n",
        "\n",
        "# Iteratively merge remaining DataFrames\n",
        "for idx, df in enumerate(dataframes[1:], 2):\n",
        "    try:\n",
        "        # Perform outer join to preserve all data\n",
        "        merged_df = pd.merge(merged_df, df,\n",
        "                           on=['date', 'city'],\n",
        "                           how='outer',\n",
        "                           suffixes=('', f'_dup_{idx}'))\n",
        "    except Exception as merge_error:\n",
        "        print(f\"‚ö†Ô∏è Merge error at file {idx}: {str(merge_error)[:50]}\")\n",
        "\n",
        "print(f\"‚úÖ Merging complete!\")\n",
        "print(f\"üìä Final merged dataset: {merged_df.shape[0]:,} rows √ó {merged_df.shape[1]} columns\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "MEHbn1Z3FJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After running this above block, my project transitions from 28 separate CSV files to one consolidated DataFrame, `merged_df`, containing all available records across dates and cities. I now know how many files were found, how many loaded successfully, and the final count of rows and columns in the merged dataset. The column names are standardized, the dates are converted to proper datetime format, and the `source_file` field tracks all data lineage. This allows me to move on to data cleaning and exploratory data analysis on a single, consistent dataset instead of managing multiple fragments. This directly fulfills the data handling and merging expectations in Task 1."
      ],
      "metadata": {
        "id": "WCmdrZhL9d2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.3 Display the first & last 5 rows to get a quick look at the marged data structure and content**\n",
        "\n",
        "I load the main India air quality CSV into a single pandas DataFrame. This gives me a clear visual sense of what the raw data looks like. Using `low_memory=False` prevents mixed-type warnings and helps pandas determine consistent data types throughout the file. This is important for a large dataset that will be cleaned, explored, and modeled. The formatted section header and separators clearly mark this as a ‚Äúfirst look‚Äù at the data. This way, I can easily see where the raw inspection begins. Displaying the DataFrame directly shows me the first and last few rows, column names, and basic structure in one view. This helps me identify obvious issues, such as unexpected columns, unusual values, or formatting problems before I spend time on more complex processing."
      ],
      "metadata": {
        "id": "Wq1fwVAHF1bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a Pandas DataFrame\n",
        "# The 'low_memory=False' argument is used to prevent mixed-type column warnings\n",
        "# for large datasets, ensuring correct data type inference.\n",
        "df = pd.read_csv(file_path, low_memory=False)  # Reloads with low_memory=False - prevents dtype warnings, ensures consistent column type inference across all chunks, better for large files\n",
        "\n",
        "print(\"\\n\\n\\n\")  # Adds vertical spacing - creates visual separation for readability\n",
        "print(\"_\"*300)  # Prints long separator - clearly demarcates new section\n",
        "display(Markdown(\"#**First Five rows & Last Five rows summary of my dataset:**\"))  # Section header - labels the preview output for documentation\n",
        "print(\"_\"*300)  # Bottom separator - completes visual framing\n",
        "df  # Displays entire DataFrame - It shows first 5 and last 5 rows by default with column summary, allows quick inspection of data structure and content"
      ],
      "metadata": {
        "id": "D9K8X8VuF3a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this above cell runs, `df` contains the complete contents of my air quality CSV with stable data types. The notebook displays a well-organized preview of the dataset. I can now see the actual city names, date format, pollutant columns, and any obvious anomalies right away. No changes have been made yet, but the data is loaded, visible, and ready for systematic cleaning and exploratory analysis in the next steps."
      ],
      "metadata": {
        "id": "2hXQ-RoS-KVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.3.1 Visualize all the pollutants by a dot plot (Scatter)**\n",
        "\n",
        "In this step, I create a quick visual overview of all the key pollutant variables to understand their overall behavior before diving deeper into data analysis. By going through a list of pollutant column names and drawing an interactive scatter plot for each one, I can quickly see the range of values, spikes, gaps, and potential outliers across the entire dataset. Using the DataFrame index on the x-axis allows me to look at the sequence of records without committing to a specific time or city filter; this is helpful for an initial check. Plotly's interactive features, like hovering and zooming, let me focus on dense areas, examine extreme points, and see exact values when needed. Coloring each plot based on pollutant levels with a continuous scale creates a visual gradient that emphasizes high-concentration episodes. This gives me early insights into which pollutants vary the most or show extreme events."
      ],
      "metadata": {
        "id": "nZpATf9XoENl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list containing the names of all pollutant columns you want to plot\n",
        "pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene']  # List of column names\n",
        "\n",
        "# Iterate through each pollutant in the list to generate a plot for it\n",
        "for pollutant in pollutants:  # Start the loop assigning the current item to the variable 'pollutant'\n",
        "    # Create a scatter plot for the current pollutant\n",
        "    fig = px.scatter(  # Initialize the figure object\n",
        "        df,  # Specify the dataframe to use\n",
        "        x=df.index,  # Set the x-axis to the dataframe index\n",
        "        y=pollutant,  # Set the y-axis to the current pollutant column\n",
        "        title=f'{pollutant} Dot Plot (Hover/Zoom)',  # Create a dynamic title using an f-string\n",
        "        color=pollutant,  # Color the data points based on the pollutant's value\n",
        "        color_continuous_scale='Viridis'  # Apply the 'Viridis' color scale\n",
        "    )  # Close the scatter function\n",
        "\n",
        "    # Update the visual properties of the plot markers\n",
        "    fig.update_traces(marker={'size': 8})  # Set the marker size to 8\n",
        "\n",
        "    # Render the plot\n",
        "    fig.show()  # Display the figure"
      ],
      "metadata": {
        "id": "9Wc5gdiYoDks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After completing this above cell loop, I obtain a set of interactive dot plots‚Äîone for each pollutant‚Äîcovering the entire dataset. I do not change anything in `df`, but my understanding improves. I can visually identify pollutants with frequent extreme values, sections with missing or near-zero data, and overall dispersion. This information helps me decide which variables may require special cleaning, transformation, or focused analysis in the next steps."
      ],
      "metadata": {
        "id": "xg68JnWX-l1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.4 Data Preprocessing**"
      ],
      "metadata": {
        "id": "AQWa3f5eGC4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4.1 Check for missing values across all columns**\n",
        "In this step, I am testing how reliable my dataset is before I start any modeling. I use `df.info()` and a `missing_summary` table to measure where and how much data is missing. This helps me make informed decisions about whether to fill in missing values or drop columns. I rank columns by the percentage of missing data to find which features are too incomplete to trust and which can still be useful. This directly impacts the stability and interpretability of my model. The matrix plot and missingness-correlation heatmap show whether the missing data is random or follows a pattern. If entire groups of rows or specific pairs of pollutants are missing together, it indicates issues with the sensors or stations. This caution is important because blindly removing rows might eliminate crucial data. Understanding this is vital for creating a cleaning strategy that maintains real pollution patterns while improving data quality. This ensures my AQI forecasts remain solid."
      ],
      "metadata": {
        "id": "4EPRuG2DGNrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(\"# **DATA CLEANING**\"))  # Main section header - marks beginning of data cleaning phase\n",
        "print(\"=\"*100)  # Separator line - visual section break\n",
        "display(Markdown(\"## **Handling Missing Values**\"))  # Subsection header - focuses on missing data analysis\n",
        "\n",
        "display(Markdown(\"### Data Types and Non-Null Counts (df.info())\"))  # Labels info output - documents what follows\n",
        "df.info(verbose=True, show_counts=True)  # Displays column metadata - shows all columns with dtypes, non-null counts, and memory usage for comprehensive overview\n",
        "print(\"\\n\\n\")  # Adds spacing - improves visual separation\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
        "})  # Creates missing data summary - calculates absolute count and percentage of nulls per column for quantitative assessment\n",
        "\n",
        "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]  # Filters to relevant columns - shows only columns with actual missing data, removes clutter\n",
        "missing_summary = missing_summary.sort_values('Missing_Percentage', ascending=False)  # Sorts by severity - prioritizes columns with highest missingness for attention\n",
        "\n",
        "print(\"\\nMissing Values Summary:\")  # Labels output - identifies the summary table\n",
        "print(missing_summary.to_string())  # Displays summary table - shows which columns need imputation or removal decisions\n",
        "print(f\"\\nTotal columns with missing values: {len(missing_summary)}\")  # Column count - quantifies scope of missing data problem\n",
        "print(f\"Total missing values in dataset: {df.isnull().sum().sum()}\")  # Total null count - provides overall data completeness metric\n",
        "\n",
        "plt.figure(figsize=(24, 22)) # Creates a new matplotlib figure container with specified dimensions (width=24, height=22).\n",
        "\n",
        "msno.matrix(df,              # Calls the missingno library's matrix plot function on the DataFrame 'df'.\n",
        "            sparkline=False, # Disables the \"sparkline\" summary graph usually shown on the right side.\n",
        "            fontsize=10,     # Sets the font size for the column names displayed along the bottom/top.\n",
        "            color=(0.25, 0.65, 0.65), # Sets the bar color using RGB values (a teal/turquoise shade).\n",
        "            figsize=(24,22)) # Redundantly sets figure size again (often overrides the plt.figure call above).\n",
        "\n",
        "plt.title('Missing Data Matrix\\n(White = Missing | Dark = Present)\\n59062 Facilities √ó 16 Variables', # Adds the main title text with newlines (\\n) for formatting.\n",
        "          fontsize=16,       # Sets the font size of the title text.\n",
        "          weight='bold',     # Makes the title text bold.\n",
        "          pad=30)            # Adds 30 points of padding (space) between the title and the top of the plot.\n",
        "\n",
        "plt.xlabel('Columns', fontsize=12) # Labels the X-axis as 'Columns' with size 12 font.\n",
        "plt.ylabel('Facilities (rows)', fontsize=12) # Labels the Y-axis as 'Facilities (rows)' with size 12 font.\n",
        "\n",
        "plt.show() # Renders the final plot and displays it in the output.\n",
        "\n",
        "# Initialize the Plotly Figure with a Heatmap\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    # Calculate the correlation matrix of the missing values (1=missing, 0=present)\n",
        "    z=df.isnull().corr().values,\n",
        "\n",
        "    # Set the X and Y axes to the column names\n",
        "    x=df.columns,\n",
        "    y=df.columns,\n",
        "\n",
        "    # Use the Red-Yellow-Blue color scale (reversed), where Red = High Correlation\n",
        "    colorscale='RdYlBu_r',\n",
        "\n",
        "    # Set the range of values for the color scale (-1 to 1)\n",
        "    zmin=-1, zmax=1,\n",
        "\n",
        "    # Provide text values for the heatmap cells\n",
        "    text=df.isnull().corr().values,\n",
        "    # Format the text to 2 decimal places\n",
        "    texttemplate=\"%{text:.2f}\",\n",
        "    # Set font size for cell text\n",
        "    textfont_size=10,\n",
        "\n",
        "    # Define custom hover text format\n",
        "    hovertemplate=\"<b>%{x}</b> vs <b>%{y}</b><br>Correlation: <b>%{z:.3f}</b><extra></extra>\",\n",
        "\n",
        "    # Configure the color bar legend\n",
        "    colorbar=dict(title=\"Correlation\", thickness=15)\n",
        "))\n",
        "\n",
        "# Update the layout settings\n",
        "fig.update_layout(\n",
        "    # Set the chart title with a subtitle\n",
        "    title=\"Missingness Correlation Heatmap<br><sub>Red = Missing Together | Blue = Independent</sub>\",\n",
        "\n",
        "    # Center the title\n",
        "    title_x=0.5,\n",
        "\n",
        "    # Use the dark theme\n",
        "    template=\"plotly_dark\",\n",
        "\n",
        "    # Set dimensions\n",
        "    height=700,\n",
        "\n",
        "    # Set global font settings\n",
        "    font=dict(family=\"Arial\", color=\"white\"),\n",
        "\n",
        "    # Angle the x-axis labels to prevent overlap\n",
        "    xaxis_tickangle=45,\n",
        "\n",
        "    # Add margin to the top to fit the annotation comfortably\n",
        "    margin=dict(t=100)\n",
        ")\n",
        "\n",
        "# Add the annotation text box\n",
        "fig.add_annotation(\n",
        "    text=\"High values (>0.7) = columns fail together\", # The message\n",
        "    xref=\"paper\", yref=\"paper\", # Use relative coordinates (0-1)\n",
        "\n",
        "    # --- CHANGED: Moved to Top Right to avoid overlapping Y-axis labels ---\n",
        "    x=1.0, y=1.05,  # 1.0 is far right, 1.05 is slightly above the plot area\n",
        "\n",
        "    showarrow=False, # No arrow\n",
        "    font_size=13, # Text size\n",
        "    bgcolor=\"#dc2626\", # Red background\n",
        "    bordercolor=\"#b91c1c\", # Darker red border\n",
        "    borderpad=8 # Padding inside the box\n",
        ")\n",
        "\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "BE90QYlQGGvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After running this above cell, my DataFrame `df` hasn‚Äôt changed, but my understanding of it has improved significantly. I now have a ranked table of missing data, a visual matrix that displays how gaps are distributed across rows, and a correlation map showing which columns often have missing values at the same time. I can identify which variables are high-risk, which are safe to fill in, and where the missing data shows a pattern instead of being random. This influences my next steps: deciding when to drop columns, picking the right imputation methods, and clearly explaining those choices in the report."
      ],
      "metadata": {
        "id": "r9vEqpV__zyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4.2 Summary Dashboard ‚Äì most incomplete columns**\n",
        "I must take this step to decide, with evidence, which features are usable and which are too damaged by missing data to trust in my models. Simply knowing that there are NaNs is not enough; I need to see how severe the missingness is for each column. By calculating and ranking missing percentages, I can quickly identify which variables are structurally weak. Features with very high missingness can distort training, reduce sample size, or force aggressive imputation that undermines model credibility. Plotting the top missing columns compels me to confront these trade-offs openly, so any decision to drop a feature, keep it, or use a specific imputation method is based on quantified risk, not guesswork. For a Level-7 project and for AQI prediction in a health context, using poorly incomplete variables without this analysis would be methodologically unsafe and hard to defend in the report."
      ],
      "metadata": {
        "id": "jpAJsSb0HLc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_counts = df.isnull().sum().sort_values(ascending=False) # Calculates the total count of null values per column and sorts them from highest to lowest.\n",
        "missing_percent = (missing_counts / len(df) * 100).round(2) # Converts those counts into percentages of the total dataset size, rounded to 2 decimal places.\n",
        "\n",
        "top_missing = pd.DataFrame({\n",
        "    'Column': missing_counts.index[:16],       # Selects the names of the columns with the most missing data.\n",
        "    'Missing_Count': missing_counts.values[:16], # Selects the raw missing counts for those top 20 columns.\n",
        "    'Missing_%': missing_percent.values[:16]   # Selects the percentage values for those top 20 columns.\n",
        "}).reset_index(drop=True)                      # Resets the index to create a clean, standard DataFrame.\n",
        "\n",
        "fig = px.bar(top_missing,                      # Initializes a bar chart using the 'top_missing' DataFrame.\n",
        "             x='Missing_%',                    # Maps the percentage of missing values to the X-axis (bar length).\n",
        "             y='Column',                       # Maps the column names to the Y-axis (categories).\n",
        "             orientation='h',                  # Sets the bar orientation to horizontal.\n",
        "             text='Missing_%',                 # Assigns the percentage values to be displayed as text labels on the bars.\n",
        "             color='Missing_%',                # Colors the bars based on the percentage value (higher % = different color).\n",
        "             color_continuous_scale='Viridis_r', # Applies a reversed Viridis color gradient (Yellow -> Purple).\n",
        "             title='Columns with Missingness<br>') # Sets the main title and a subtitle using HTML tags.\n",
        "\n",
        "fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside') # Formats the text labels to show 1 decimal place with a '%' sign and places them outside the bars.\n",
        "\n",
        "fig.update_layout(height=700,                  # Sets the plot height to 700 pixels.\n",
        "                  yaxis={'categoryorder':'total ascending'}, # Sorts the Y-axis bars so the longest bars are at the top (matches 'ascending' logic for horizontal plots).\n",
        "                  xaxis_title=\"Percentage of Missing Values (%)\", # Labels the X-axis.\n",
        "                  yaxis_title=\"Column Name\",   # Labels the Y-axis.\n",
        "                  title_x=0.5)                 # Centers the title horizontally (0.5 = middle).\n",
        "\n",
        "fig.show() # Renders and displays the interactive Plotly chart."
      ],
      "metadata": {
        "id": "5QqAoTFeHNUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I now have a ranked table (`top_missing`) and a clear visual that reveal which columns are critically incomplete. My raw DataFrame remains the same, but my feature set is no longer assumed to be all fine; I have identified which variables I cannot responsibly treat as equal inputs. This serves as the evidence base for setting thresholds (e.g. drop above X% missing) and justifying those choices in my cleaning and modeling strategy."
      ],
      "metadata": {
        "id": "8W0IQp1RAbxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1.4.2.1 Create Backup**  \n",
        "\n",
        "I need to create a clean backup of the original DataFrame. This way, I always have a \"ground truth\" version of the dataset to compare to and roll back to if necessary. Once I start removing rows, filling in values, or creating new features, it‚Äôs hard to see exactly what has changed without an untouched copy. Having `df_original` allows me to show later, with evidence, how many rows or columns were removed, how distributions changed, and whether any cleaning step unintentionally altered the data. It also protects me from irreversible mistakes during experimentation. This is important for a rigorous workflow."
      ],
      "metadata": {
        "id": "4VAycTk1AryV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a backup of the original dataframe before cleaning\n",
        "df_original = df.copy()  # Duplicates DataFrame - preserves unmodified dataset for comparison and audit trail\n",
        "print(f\"\\nOriginal dataset shape: {df_original.shape}\")  # Displays dimensions - confirms backup integrity with rows and columns count\n"
      ],
      "metadata": {
        "id": "aeD4j9DMHmIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I now have two separate objects: `df_original`, a frozen copy of the raw dataset, and `df`, which I can modify during cleaning and transformation. The printed shape confirms that the backup is complete and matches the current data dimensions. From this point on, any structural or statistical changes apply only to `df`, while `df_original` remains my reference for validation, comparison, and recovery if needed."
      ],
      "metadata": {
        "id": "xTaCafdlAVFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.4.3 HANDLE MISSING VALUES**"
      ],
      "metadata": {
        "id": "N82-KjkM0J3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have choose the Date column as index. Choosing the 'Date' column as the index is the single most critical step in preparing time-series data for analysis, particularly for environmental datasets like Air Quality Index (AQI). This decision is driven by the fundamental nature of the data: air pollution is a physical phenomenon that evolves continuously over time, not over arbitrary row numbers.\n",
        "\n",
        "When a dataset is indexed by row numbers (0, 1, 2‚Ä¶), the computer treats every data point as being \"equidistant\" from its neighbor. This creates a dangerous fallacy when dealing with real-world sensor data, which is rarely perfect. Sensors often fail, power goes out, or networks disconnect, leading to irregular gaps in data collection. If you use a simple row index to fill missing values, the algorithm treats a gap of one hour exactly the same as a gap of five days. It simply draws a straight line between the row above and the row below. In the context of air quality, this is scientifically invalid; pollution levels might remain stable over an hour, but they can change drastically over five days due to wind, rain, or traffic patterns.\n",
        "\n",
        "By setting the 'Date' as the index, you transform the dataset into a time-aware structure. The interpolation method `method='time'` can then calculate the exact temporal distance between valid data points. It understands that a missing value at 2:00 PM should be inferred from the 1:00 PM and 3:00 PM readings, weighing them according to how much time has passed. This preserves the integrity of the data's slope and trajectory.\n",
        "\n",
        "Furthermore, in a merged dataset containing multiple cities, using a row index introduces the risk of \"data leakage.\" If the dataset transitions from Delhi‚Äôs data to Mumbai‚Äôs data without a time index, a row-based interpolation might average the last value of Delhi with the first value of Mumbai to fill a gap, creating a fictitious data point that represents neither city. A DatetimeIndex, especially when combined with proper grouping, enforces the logic that data points are only related if they are proximal in time, ensuring that your analysis reflects the reality of atmospheric physics rather than the artifacts of data storage."
      ],
      "metadata": {
        "id": "YgQhPyZc0UsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.3.1 DATA STANDARDIZATION & TEMPORAL SETUP**\n",
        "I need to standardize column names and the date index now to prevent subtle bugs and inconsistencies throughout the pipeline. Cleaning the column names into a consistent, machine-friendly format using lowercase letters, underscores, and no dots makes the later code for selection, merging, plotting, and modeling more reliable and easier to read. Relying on messy names like \"PM 2.5\" or \"AQI.Value\" would make the notebook fragile and prone to errors. Changing the `date` column into a proper datetime type, then sorting and setting it as the index, is essential because this is fundamentally a temporal dataset. Resampling, rolling statistics, interpolation, and time-aware visualizations all depend on a valid, ordered datetime index. Without this step, time-series operations and lag-based features could behave incorrectly or produce misleading results. The simple Plotly indicator is not just for show; it serves as a clear checkpoint, indicating that the dataset is now structurally standardized and ready for serious cleaning, exploratory data analysis, and modeling based on a reliable schema and time axis."
      ],
      "metadata": {
        "id": "nIFQksRp1Iwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA STANDARDIZATION & CONFIRMATION\n",
        "\n",
        "# 1. Clean Column Names\n",
        "# Convert to lowercase, replace spaces with underscores, and remove dots.\n",
        "# Example: \"PM 2.5\" -> \"pm_25\", \"AQI.Value\" -> \"aqivalue\"\n",
        "merged_df.columns = [c.lower().replace(' ', '_').replace('.', '') for c in merged_df.columns]\n",
        "\n",
        "# 2. Parse Dates\n",
        "# Convert the 'date' column to proper datetime objects.\n",
        "# 'errors=coerce' turns unparseable dates into NaT (Not a Time) instead of crashing.\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'], errors='coerce')\n",
        "\n",
        "# 3. Set Index\n",
        "# Sort chronologically and set 'date' as the index.\n",
        "# This is critical for time-series operations (resampling, interpolation, plotting).\n",
        "merged_df = merged_df.sort_values('date').set_index('date')\n",
        "\n",
        "\n",
        "# VISUAL CONFIRMATION\n",
        "\n",
        "# Create a visual \"Success\" indicator using Plotly\n",
        "fig = go.Figure(go.Indicator(\n",
        "    mode=\"delta+gauge+number\",      # Show the number, a gauge bar, and a delta arrow\n",
        "    value=100,                      # Set value to 100% to indicate completion\n",
        "    delta={'reference': 0, 'position': \"top\"}, # Show change from 0 (visual flair)\n",
        "    gauge={\n",
        "        'axis': {'range': [0, 100]}, # Set gauge range from 0 to 100\n",
        "        'bar': {'color': \"#10b981\"}  # Set the bar color to green (success)\n",
        "    },\n",
        "    title={'text': \"<b>Data Standardization Complete</b><br><sub>Clean columns ‚Ä¢ Valid datetime index</sub>\"}\n",
        "))\n",
        "\n",
        "# Adjust layout for a clean look\n",
        "fig.update_layout(height=250, paper_bgcolor=\"#f8fafc\")\n",
        "\n",
        "# Display the indicator\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PxNH01TB1TEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, `merged_df` has clean, uniform column names, and the `date` column is converted to datetime, sorted, and set as the DataFrame index. Any unparseable dates are safely converted to `NaT` instead of breaking the code. The Plotly gauge visually confirms the standardization checkpoint. From this point on, all downstream code can assume a consistent naming convention and a properly ordered time index, which simplifies filtering, grouping, resampling, and feature engineering."
      ],
      "metadata": {
        "id": "c2fwncOAWNvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.3.2 TIME-BASED INTERPOLATION & VISUAL CONFIRMATION**\n",
        "I need to address the missing values in the numerical pollutant variables before modeling. Most algorithms can‚Äôt handle NaNs. More importantly, gaps in a time series can distort trends and forecasts. A simple mean or median fill ignores the time structure and may flatten real peaks or troughs in pollution. By selecting only numeric columns and then using time-based interpolation on the datetime index, I can estimate reasonable values between known observations based on the natural order of the data. The next steps of forward-fill and back-fill are essential for handling missing values at the start or end of the series, where interpolation alone lacks bounding points. This three-step approach provides a complete, numeric dataset that maintains time continuity, avoids discarding rows, and is suitable for downstream AQI modeling and seasonal analysis. Without this, I would either lose important data through row deletion or input unreliable, random estimates into the models."
      ],
      "metadata": {
        "id": "Pfp-QPE22G1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical columns only (interpolation requires numbers, not text like 'City')\n",
        "numeric_cols = merged_df.select_dtypes('number').columns\n",
        "\n",
        "# Apply a robust 3-step filling strategy to the numerical columns:\n",
        "merged_df[numeric_cols] = (\n",
        "    merged_df[numeric_cols]\n",
        "    .interpolate(method='time')   # Step 1: Fill gaps linearly based on the time index (respects time distance)\n",
        "    .ffill()                        # Step 2: Forward fill to catch any missing values at the very start (before first valid point)\n",
        "    .bfill()                        # Step 3: Backward fill to catch any missing values at the very end\n",
        ")\n",
        "\n",
        "# 2. VISUALIZATION\n",
        "\n",
        "import plotly.graph_objects as go  # Import Plotly Graph Objects for detailed chart control\n",
        "\n",
        "# Create the figure with an Indicator trace (Gauge chart)\n",
        "fig = go.Figure(go.Indicator(\n",
        "    mode = \"gauge+number+delta\",    # Display the Gauge bar, the Number value, and the Delta indicator\n",
        "    value = 100,                    # Set current value to 100% (indicating task completion)\n",
        "    domain = {'x': [0, 1], 'y': [0, 1]}, # Use the full figure area\n",
        "\n",
        "    # Configure the main title with HTML styling for size and color\n",
        "    title = {\n",
        "        'text': \"<b>Time-Based Interpolation</b><br><span style='font-size:0.9em;color:#94a3b8'>All numerical gaps intelligently filled</span>\",\n",
        "        'font': {'size': 18}\n",
        "    },\n",
        "\n",
        "    # Configure the Delta (comparison) indicator\n",
        "    delta = {'reference': 0, 'increasing': {'color': \"#10b981\"}}, # Reference 0 shows full growth, green color\n",
        "\n",
        "    # Configure the semi-circle Gauge\n",
        "    gauge = {\n",
        "        'axis': {'range': [0, 100], 'tickwidth': 1, 'tickcolor': \"white\"}, # Axis from 0 to 100\n",
        "        'bar': {'color': \"#10b981\"},    # Main progress bar color (Emerald Green)\n",
        "        'bgcolor': \"#1e293b\",           # Background color of the gauge track (Dark Slate)\n",
        "        'borderwidth': 2,               # Width of border around gauge\n",
        "        'bordercolor': \"#334155\",       # Border color\n",
        "\n",
        "        # Color steps for the track background (Darker slate -> Greenish hint at end)\n",
        "        'steps': [\n",
        "            {'range': [0, 70], 'color': '#334155'},\n",
        "            {'range': [70, 100], 'color': '#166534'}\n",
        "        ],\n",
        "\n",
        "        # Threshold line to visually mark the \"Goal\" at 100%\n",
        "        'threshold': {\n",
        "            'line': {'color': \"#10b981\", 'width': 8},\n",
        "            'thickness': 0.75,\n",
        "            'value': 100\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # Configure the central number text\n",
        "    number = {'suffix': \"%\", 'font': {'size': 60, 'color': \"#e2e8f0\"}} # White text with % symbol\n",
        "))\n",
        "\n",
        "# Update global layout settings for the Dark Theme look\n",
        "fig.update_layout(\n",
        "    paper_bgcolor = \"#0f172a\",      # Very dark blue/slate background for the entire chart\n",
        "    font = {'color': \"#e2e8f0\", 'family': \"Arial\"}, # Global font settings (light grey text)\n",
        "    height = 320,                   # Set figure height\n",
        "    margin = dict(t=80, b=20, l=40, r=40) # Margins to frame the chart nicely\n",
        ")\n",
        "\n",
        "# Add a text annotation at the bottom for extra context\n",
        "fig.add_annotation(\n",
        "    text=\"Success: 100% of missing values filled using time-aware interpolation\",\n",
        "    xref=\"paper\", yref=\"paper\",     # Use relative coordinates (0-1)\n",
        "    x=0.5, y=-0.1,                  # Position: Center X, slightly below bottom Y\n",
        "    showarrow=False,                # No arrow pointing to data\n",
        "    font=dict(size=13, color=\"#94a3b8\") # Smaller, muted text color\n",
        ")\n",
        "\n",
        "# Render the interactive chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "J1jtJMz62HZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, all numeric columns in `merged_df` no longer have NaNs. Internal gaps are filled using time-aware interpolation, and edge gaps are handled with forward and backward fills. The structure of the DataFrame (rows and columns) remains the same, but the quality has improved. Every record now has complete pollutant values, making the dataset ready for thorough EDA, correlation analysis, and machine learning models without needing more missing-value handling."
      ],
      "metadata": {
        "id": "mqNWdOzwXEm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.4 FINAL CLEAN-UP & INTERACTIVE SUCCESS REPORT**\n",
        "I need to reset the index and check for missing values. This way, I can finish the cleaning phase and confirm that the dataset is ready for analysis. Changing `date` back from an index to a regular column makes it easier to use in plots, groupings, and feature engineering. It is better for `date` to be an explicit field rather than a hidden index. Checking for remaining nulls in all numeric columns is a necessary step. If I skip this check, I could mistakenly include leftover NaNs in models and visualizations. This could lead to errors or unrecognized bias. The gauge-style indicator and annotation make this technical check clear and visible, showing that all numeric gaps are filled and the dataset is usable. This is important for my confidence and for proving to the marker that the cleaning process has a clear endpoint and is not just a series of random steps."
      ],
      "metadata": {
        "id": "WVCL5Aej5_19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset Index\n",
        "# Convert the 'date' index back into a regular column so it's easier to use in future plots.\n",
        "# 'inplace=True' modifies the dataframe directly without needing to reassign it.\n",
        "merged_df.reset_index(inplace=True)\n",
        "\n",
        "# Verify Data Cleanliness\n",
        "# Select all numerical columns and sum up any remaining null (NaN) values.\n",
        "# The result 'remaining' should be 0 if our cleaning process worked perfectly.\n",
        "remaining = merged_df.select_dtypes('number').isnull().sum().sum()\n",
        "\n",
        "# CREATE DASHBOARD (Plotly Indicator)\n",
        "# Initialize a Plotly Figure with an Indicator trace (Gauge chart)\n",
        "fig = go.Figure(go.Indicator(\n",
        "    # MODE: Display the Number, the Gauge bar, and the Delta (change) indicator\n",
        "    mode=\"number+gauge+delta\",\n",
        "\n",
        "    # VALUE: Set the completion status to 100% (indicating success)\n",
        "    value=100,\n",
        "\n",
        "    # DELTA: Configure the change indicator\n",
        "    # reference=remaining: Compares 100% against the 'remaining' missing count (ideally 0)\n",
        "    # position=\"top\": Places the delta arrow above the main number\n",
        "    delta={'reference': remaining, 'relative': False, 'position': \"top\"},\n",
        "\n",
        "    # GAUGE: Configure the semi-circle bar\n",
        "    # range=[0, 100]: Standard percentage scale\n",
        "    # bar color #10b981: Sets the progress bar to Emerald Green\n",
        "    gauge={'axis': {'range': [0, 100]}, 'bar': {'color': \"#10b981\"}},\n",
        "\n",
        "    # NUMBER: Configure the central text\n",
        "    # suffix=\"%\": Adds percentage sign\n",
        "    # font size 70: Makes it large and readable\n",
        "    number={'suffix': \"%\", 'font': {'size': 70}},\n",
        "\n",
        "    # TITLE: HTML-styled chart title with a subtitle for context\n",
        "    title={'text': \"<b>Data Cleaning Complete</b><br><span style='font-size:0.9em;color:#94a3b8'>All numerical gaps filled ‚Ä¢ Ready for analysis</span>\"}\n",
        "))\n",
        "\n",
        "# UPDATE LAYOUT\n",
        "# Customize the overall figure appearance for a professional dark theme\n",
        "fig.update_layout(\n",
        "    height=300,                     # Set figure height\n",
        "    paper_bgcolor=\"#0f172a\",        # Dark blue/slate background color\n",
        "    font={'color': \"#e2e8f0\", 'family': \"Arial\"}, # Light grey font color for contrast\n",
        "    margin=dict(t=80, b=20, l=40, r=40) # Set margins to frame the chart\n",
        ")\n",
        "\n",
        "# ADD ANNOTATION\n",
        "# Adds a text box at the bottom to explicitly state the result\n",
        "fig.add_annotation(\n",
        "    # Text string showing the count of missing values (formatted with commas)\n",
        "    text=f\"Remaining missing values: <b>{remaining:,}</b> ‚Üí <span style='color:#10b981'>Perfect</span>\",\n",
        "    xref=\"paper\", yref=\"paper\",     # Use relative coordinates (0-1)\n",
        "    x=0.5, y=-0.15,                 # Position: Center X, below the chart Y\n",
        "    showarrow=False,                # No arrow pointing to specific data\n",
        "    font=dict(size=15),             # Font size\n",
        "    bgcolor=\"#166534\",              # Dark green background box\n",
        "    borderpad=10                    # Padding inside the box\n",
        ")\n",
        "\n",
        "# Display the interactive chart\n",
        "fig.show()\n",
        "\n",
        "# PREVIEW DATA\n",
        "# Display the first 5 rows of the cleaned dataframe to verify the result\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "7i7yLgha2M59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "The negative number in the delta (\"-1,594,574\") is not saying df have negative missing values‚Äîthat would be impossible. Instead, it's showing the reduction (change) in missing values after cleaning:\n",
        "\n",
        "Before cleaning: There were 1,594,574 missing values in the dataset.\n",
        "After cleaning: 0 missing values remain.\n",
        "Delta = change = -1,594,574 (meaning you successfully filled all of them).\n",
        "\n",
        "After this step, `merged_df` now has `date` back as a standard column. All numeric columns have been confirmed to contain zero missing values (`remaining == 0`). I also have a visual dashboard element showing that cleaning is complete. The head preview of `merged_df` displays the final structure, ready for analysis, EDA, feature engineering, and modeling."
      ],
      "metadata": {
        "id": "B2E-aTC-8-nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1.4.4.1 Checking for Remaining Missing values**\n",
        "\n",
        "This step checks that the cleaning pipeline has removed all missing values, including those in non-numeric or newly created columns. It ensures clear methods and avoids leftover errors that could disrupt future visualizations or models."
      ],
      "metadata": {
        "id": "cHOwQR3ZYFD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which specific columns still have missing data\n",
        "print(merged_df.isnull().sum())"
      ],
      "metadata": {
        "id": "CEdFUKFP7P1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "While the data itself does not change, this creates a clear report on data completeness. If the output is zero, the data is confirmed as ready. If it is not, it identifies specific columns that still need focused fixing or removal."
      ],
      "metadata": {
        "id": "HN-F8vw4YW7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.5 FINAL DATA QUALITY CHECK & SUMMARY**\n",
        "I need to clearly mark the end of data cleaning and show that the dataset is truly ready for EDA, not just assume it. The Markdown header and the 100% indicator create a visible checkpoint in the notebook. This lets anyone reading it see where the cleaning phase ends and the analysis phase starts. This is important for assessment and reproducibility. It shows that I treat data preparation as a separate, completed stage. The styled preview of the first few rows provides a quick check to ensure the final structure, column types, and value ranges look reasonable after all transformations. Without this clear sign-off, it would be harder to show that the dataset is in a stable, consistent state before I start drawing conclusions from it."
      ],
      "metadata": {
        "id": "KVpJMSj8Ht9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual Success Indicator\n",
        "# Initialize a Plotly Figure with a simple 'Indicator' (Big Number) trace\n",
        "fig = go.Figure(go.Indicator(\n",
        "    mode=\"number\",          # Display only the numerical value (no gauge bar needed here)\n",
        "    value=100,              # Hardcoded to 100 to represent \"100% Complete\"\n",
        "    number={'suffix': \"%\", 'font': {'size': 80}}, # Add a '%' sign and make the text large\n",
        "\n",
        "    # Create a two-line title using HTML: Bold main text, smaller subtitle\n",
        "    title={'text': \"<b>Data Cleaning Complete</b><br><span style='font-size:0.8em'>All gaps filled ‚Ä¢ Ready for EDA</span>\"}\n",
        "))\n",
        "\n",
        "# Styling the Indicator\n",
        "# Customize the layout to match a dark theme (professional look)\n",
        "fig.update_layout(\n",
        "    height=250,             # Set a compact height\n",
        "    paper_bgcolor=\"#0f172a\", # Dark blue/slate background color\n",
        "    font_color=\"white\"      # White text for contrast against the dark background\n",
        ")\n",
        "\n",
        "# Render the interactive chart\n",
        "fig.show()\n",
        "\n",
        "# Data Preview\n",
        "# Display the first 3 rows of the final dataframe to verify structure\n",
        "# .style accesses the Pandas Styler for visual formatting\n",
        "# .set_table_attributes sets the font size for readability\n",
        "# .background_gradient adds a heat-map effect (darker blue = higher values) to spot trends quickly\n",
        "df.head(3).style.set_table_attributes('style=\"font-size:14px\"').background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "yT0z56-YHu9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, the data itself remains the same, but the notebook includes a clear ‚ÄúData Cleaning Complete‚Äù marker and a small, formatted snapshot of the cleaned DataFrame. This gives me and the marker immediate visual confirmation of the final structure and reinforces that subsequent plots and models are based on a clearly defined, cleaned dataset."
      ],
      "metadata": {
        "id": "eb8gBmqcY7YY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.6 OUTLIER DETECTION & HANDLING (IQR METHOD)**\n",
        "I need to identify and limit outliers to prevent a few extreme pollutant readings from skewing the statistics and destabilizing my models. In air quality data, instruments can show spikes due to sensor faults, local issues, or data-entry mistakes. If I ignore these, algorithms like linear regression or tree-based models may fit too closely to these extremes or inaccurately estimate typical AQI levels. By using the IQR method on numeric columns, I can set clear, distribution-based limits (Q1 - 1.5¬∑IQR, Q3 + 1.5¬∑IQR) and cap values beyond these thresholds instead of removing rows. This capping preserves the time series and sample size, which is critical for analyzing temporal patterns, while minimizing the impact of unrealistic extremes. The Plotly indicator and annotation show how many values were adjusted, providing me with a clear view of the effects. Without this step, my subsequent correlations, regressions, and forecasts could be distorted by a few outliers that don't represent normal pollution behavior, which could weaken both the scientific validity and credibility of my conclusions."
      ],
      "metadata": {
        "id": "l0Kz0Rkj-bUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a status message to track progress\n",
        "print(\"Detecting and capping outliers using IQR method...\")\n",
        "\n",
        "# IDENTIFY NUMERIC DATA\n",
        "# Outlier detection only applies to numbers, so we isolate numeric columns\n",
        "num_cols = df.select_dtypes(include='number').columns\n",
        "\n",
        "# CALCULATE STATISTICAL BOUNDS (The IQR Logic)\n",
        "# Calculate the 25th percentile (Q1) and 75th percentile (Q3) for all columns\n",
        "Q1 = df[num_cols].quantile(0.25)\n",
        "Q3 = df[num_cols].quantile(0.75)\n",
        "# Calculate the Interquartile Range (IQR), which represents the middle 50% of data\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# APPLY CAPPING (WINSORIZATION)\n",
        "# Create a copy of the dataframe to avoid modifying the original 'df' directly yet\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Use the .clip() method to cap values.\n",
        "# Values lower than (Q1 - 1.5*IQR) are replaced with that lower bound.\n",
        "# Values higher than (Q3 + 1.5*IQR) are replaced with that upper bound.\n",
        "# This keeps the data distribution intact without deleting rows (better for time series).\n",
        "df_clean[num_cols] = df[num_cols].clip(lower=Q1 - 1.5*IQR, upper=Q3 + 1.5*IQR, axis=1)\n",
        "\n",
        "# CALCULATE IMPACT\n",
        "# Compare the original dataframe with the cleaned one to count how many values changed\n",
        "outliers_capped = (df[num_cols] != df_clean[num_cols]).sum().sum()\n",
        "\n",
        "\n",
        "# VISUAL SUCCESS INDICATOR\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Initialize the Plotly Indicator figure\n",
        "fig = go.Figure(go.Indicator(\n",
        "    # MODE: Show the big number and a 'delta' comparison\n",
        "    mode=\"number+delta\",\n",
        "\n",
        "    # VALUE: The total count of outliers that were fixed\n",
        "    value=outliers_capped,\n",
        "\n",
        "    # NUMBER STYLE: Make it large and Red (#dc2626) to highlight the correction\n",
        "    number={'font': {'size': 70, 'color': '#dc2626'}},\n",
        "\n",
        "    # DELTA: Shows change relative to 0 (effectively just showing the number again for emphasis)\n",
        "    delta={'reference': 0, 'position': \"top\"},\n",
        "\n",
        "    # TITLE: HTML-styled header explaining what happened\n",
        "    title={'text': \"<b>Outlier Capping Complete</b><br><span style='font-size:0.9em;color:#94a3b8'>Extreme values safely capped using IQR method</span>\"}\n",
        "))\n",
        "\n",
        "# LAYOUT: Style the chart background to match a dark professional theme\n",
        "fig.update_layout(\n",
        "    height=300,                     # Set height\n",
        "    paper_bgcolor=\"#0f172a\",        # Dark blue background\n",
        "    font={'color': \"white\", 'family': \"Arial\"}, # White text\n",
        "    margin=dict(t=80, b=20, l=40, r=40) # Margins\n",
        ")\n",
        "\n",
        "# ANNOTATION: Add a summary box at the bottom\n",
        "fig.add_annotation(\n",
        "    text=f\"<b>{outliers_capped:,}</b> extreme values capped ‚Üí Model-ready data\",\n",
        "    xref=\"paper\", yref=\"paper\",     # Relative positioning\n",
        "    x=0.5, y=-0.15,                 # Center bottom\n",
        "    showarrow=False,                # No arrow\n",
        "    font=dict(size=15),             # Font size\n",
        "    bgcolor=\"#7f1d1d\",              # Dark red background box\n",
        "    borderpad=10                    # Padding inside the box\n",
        ")\n",
        "\n",
        "# Display the interactive dashboard component\n",
        "fig.show()\n",
        "\n",
        "# COMMIT CHANGES\n",
        "# Overwrite the original dataframe with the cleaned version\n",
        "df = df_clean\n",
        "# Print final confirmation\n",
        "print(f\"Outliers capped: {outliers_capped:,} values adjusted\")"
      ],
      "metadata": {
        "id": "cWmyo4Gg-s_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, `df_clean` becomes the new `df`, where all numeric columns have been limited to IQR-based bounds. The number of adjusted values (`outliers_capped`) is calculated and displayed, so I know exactly how many points were affected. The overall row and column structure is maintained, but extreme pollutant values have been controlled, giving me a more robust, model-ready dataset that still captures realistic variation without being influenced by rare spikes."
      ],
      "metadata": {
        "id": "VXw43CPyZifp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.7 Feature Engineering:**\n",
        "I need to standardize the date and column names and create these time-related features because time plays a crucial role in air quality. My models and exploratory data analysis need to reflect that structure. Normalizing column names helps avoid fragile code and KeyErrors later. If I have mixed cases, spaces, and dots, simple tasks like selection, merging, or plotting can break unexpectedly. Changing `date` to a proper datetime type is important so I can safely use `.dt` accessors and build time-aware features. The new variables‚Äî`year`, `month`, `weekday`, `is_weekend`, and `season`‚Äîtransform raw timestamps into meaningful patterns. They allow me to explore questions such as, ‚ÄúDoes AQI systematically worsen in winter?‚Äù, ‚ÄúAre weekends cleaner than weekdays?‚Äù, and ‚ÄúHow do different seasons behave across cities?‚Äù. Without these engineered features, I would not be able to measure seasonality and calendar effects effectively. My models would have to deduce temporal structure from just one raw date field, which is much less effective and harder to interpret."
      ],
      "metadata": {
        "id": "8-8x1AhA_VmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize Column Names\n",
        "# Convert all column names to lowercase to match the code's expectation\n",
        "df.columns = [c.lower().strip().replace(' ', '_').replace('.', '') for c in df.columns]\n",
        "\n",
        "# Ensure 'date' is a Datetime Object (Fixes .dt accessor error)\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# Create Features\n",
        "df['year']       = df['date'].dt.year         # Extract Year\n",
        "df['month']      = df['date'].dt.month        # Extract Month\n",
        "df['weekday']    = df['date'].dt.day_name()   # Extract Day Name\n",
        "df['is_weekend'] = df['date'].dt.weekday >= 5 # Boolean Flag for Weekend\n",
        "\n",
        "# Map Months to Indian Seasons\n",
        "df['season'] = df['month'].map({\n",
        "    12:'Winter', 1:'Winter', 2:'Winter',\n",
        "    3:'Spring', 4:'Spring', 5:'Spring',\n",
        "    6:'Summer', 7:'Summer', 8:'Summer',\n",
        "    9:'Monsoon', 10:'Monsoon', 11:'Monsoon'\n",
        "})\n",
        "\n",
        "# VISUAL CONFIRMATION DASHBOARD\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add Success Checkmark (Using Text/Emoji for reliability)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0.5], y=[1.2],\n",
        "    mode=\"text\",  # Changed to text mode only\n",
        "    text=[\"‚úÖ\"],  # Use the emoji directly\n",
        "    textfont=dict(size=80), # Make it big\n",
        "    hoverinfo=\"none\"\n",
        "))\n",
        "\n",
        "# Add Summary Table\n",
        "fig.add_trace(go.Table(\n",
        "    header=dict(\n",
        "        values=[\"<b>New Feature</b>\", \"<b>Description</b>\"],\n",
        "        fill_color=\"#1e293b\", font=dict(color=\"white\", size=14), align=\"left\"\n",
        "    ),\n",
        "    cells=dict(\n",
        "        values=[\n",
        "            ['year', 'month', 'weekday', 'is_weekend', 'season'],\n",
        "            ['Calendar year', 'Month number', 'Day name', 'Weekend flag', 'Indian season']\n",
        "        ],\n",
        "        fill_color=\"#0f172a\", font=dict(color=\"#e2e8f0\"), align=\"left\", height=30\n",
        "    )\n",
        "))\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    title={\n",
        "        'text': \"<b>Feature Engineering Complete</b><br><span style='font-size:0.9em;color:#94a3b8'>5 intelligent temporal features added</span>\",\n",
        "        'x': 0.5, 'y': 0.95\n",
        "    },\n",
        "    paper_bgcolor=\"#0f172a\", plot_bgcolor=\"#0f172a\",\n",
        "    font=dict(family=\"Arial\", color=\"white\"),\n",
        "    height=400, margin=dict(t=80, b=20, l=20, r=20),\n",
        "    showlegend=False, xaxis=dict(visible=False), yaxis=dict(visible=False)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QEtYEtfM_ecJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, `df` has clean, consistent column names, a properly typed `date` column, and five new time-related features (`year`, `month`, `weekday`, `is_weekend`, `season`) added to every row. The Plotly ‚ÄúFeature Engineering Complete‚Äù dashboard shows the new variables and their roles. My dataset is now richer and better reflects how AQI actually behaves over time, allowing for more insightful exploratory data analysis and more informative models."
      ],
      "metadata": {
        "id": "_Q3TtHGRZ-1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.8 CATEGORICAL ENCODING: PREPARING TEXT DATA FOR MACHINE LEARNING**\n",
        "I need to change `aqi_bucket` into a clean, ordered numeric feature. Many machine learning models cannot work directly with text categories, and we would lose the natural order of AQI severity. If I leave NaNs in this column, the OrdinalEncoder will either fail or produce inconsistent codes. Filling in missing buckets with a neutral middle category, ‚ÄúModerate,‚Äù is a practical way to keep all rows and avoid crashes. By defining an explicit order from *Good* to *Severe*, I ensure the encoded values reflect increasing pollution severity. This is important for understanding results and for models that benefit from an ordinal structure. Without this step, I couldn't use `aqi_bucket` as a target or an input feature in any numerical model, which would reduce my ability to analyze and predict air quality levels."
      ],
      "metadata": {
        "id": "8YlYbQ64BPVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill Missing Values in the Categorical Column\n",
        "# We fill NaNs with \"Moderate\" (the middle value) so the encoder doesn't crash.\n",
        "if 'aqi_bucket' in df.columns:\n",
        "    # Check how many are missing\n",
        "    missing_buckets = df['aqi_bucket'].isnull().sum()\n",
        "    if missing_buckets > 0:\n",
        "        print(f\"‚ö†Ô∏è Found {missing_buckets} missing AQI buckets. Filling with 'Moderate'...\")\n",
        "        df['aqi_bucket'] = df['aqi_bucket'].fillna('Moderate')\n",
        "\n",
        "    # Initialize the Encoder\n",
        "    # We define the specific order so the model knows Good < Severe\n",
        "    encoder = OrdinalEncoder(categories=[['Good','Satisfactory','Moderate','Poor','Very Poor','Severe']])\n",
        "\n",
        "    # Apply Transformation\n",
        "    # Now that NaNs are gone, this will work perfectly\n",
        "    df['aqi_bucket_encoded'] = encoder.fit_transform(df[['aqi_bucket']]).astype(int)\n",
        "\n",
        "    # VISUAL CONFIRMATION\n",
        "    fig = go.Figure(data=go.Table(\n",
        "        header=dict(values=[\"<b>AQI Bucket</b>\", \"<b>Encoded Value</b>\"],\n",
        "                    fill_color=\"#242927\", font=dict(color=\"white\", size=15)),\n",
        "        cells=dict(values=[\n",
        "            ['Good','Satisfactory','Moderate','Poor','Very Poor','Severe'],\n",
        "            [0, 1, 2, 3, 4, 5]\n",
        "        ], fill_color=\"#172554\", font=dict(color=\"#e0e7ff\"))\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"<b>Categorical Encoding Complete</b><br><sub>AQI_Bucket ‚Üí Numerical (0‚Äì5) ‚Ä¢ Ready for ML models</sub>\",\n",
        "        title_x=0.5,\n",
        "        paper_bgcolor=\"#bee8d7\",\n",
        "        height=380,\n",
        "        margin=dict(t=80, b=20, l=20, r=20)\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    print(\"‚úÖ AQI_Bucket successfully encoded!\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Error: 'aqi_bucket' column not found.\")"
      ],
      "metadata": {
        "id": "MEm-dX-bBP1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I replaced any missing `aqi_bucket` values with ‚ÄúModerate.‚Äù I also added a new numeric column, `aqi_bucket_encoded`, containing values from 0 (Good) to 5 (Severe). The original text labels are still there, but I now have a machine-readable ordinal feature that captures AQI severity for modeling and analysis."
      ],
      "metadata": {
        "id": "4Emf_a3maepg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.9 FEATURE SCALING**\n",
        "I need to standardize the pollutant features and save a clean, processed dataset so my models train reliably and my workflow is reproducible. Different pollutants are measured on different scales, such as CO versus PM2.5. If I leave them unscaled, algorithms that react to feature size can be influenced by the largest variables, hiding the true importance of others. Using `StandardScaler` to apply z-score scaling puts all pollutant features on a comparable scale, with a mean of 0 and unit variance. This stabilizes optimization and makes model coefficients and feature importances easier to understand. Filtering the pollutant list to include only existing columns prevents errors and keeps the pipeline stable in response to small changes in the schema. Writing the scaled DataFrame to `India_Air_Quality_Final_Processed.csv` is crucial. It provides me with a fixed, versioned dataset that I can load later for modeling, validation, or integrating into the Streamlit app without running all preprocessing steps again. The completion dashboard and annotation document how many features were scaled and how many rows are in the final file, increasing the transparency of the preprocessing phase."
      ],
      "metadata": {
        "id": "EIjO4eInCJEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StandardScaler for normalization\n",
        "# This standardizes features by removing the mean and scaling to unit variance (z-score)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# DEFINE FEATURES TO SCALE\n",
        "# List all potential numerical pollutant columns that need scaling\n",
        "pollutants = ['pm25','pm10','no','no2','nox','nh3','co','so2','o3','benzene','toluene','xylene']\n",
        "\n",
        "# Filter the list to ensure we only try to scale columns that actually exist in our dataframe\n",
        "# This prevents 'KeyError' crashes if a specific pollutant is missing\n",
        "pollutants = [c for c in pollutants if c in df.columns]\n",
        "\n",
        "# APPLY SCALING\n",
        "# Initialize the scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a deep copy of the dataframe to keep the original data intact (good practice)\n",
        "# We store the scaled version in 'df_scaled'\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Fit the scaler to the data (calculate mean/std) and transform it (apply formula)\n",
        "# We overwrite only the pollutant columns in the new dataframe with their scaled versions\n",
        "df_scaled[pollutants] = scaler.fit_transform(df[pollutants])\n",
        "\n",
        "# SAVE TO FILE\n",
        "# Export the final, fully processed dataset to a CSV file\n",
        "# index=False prevents saving the row numbers as a separate column\n",
        "df_scaled.to_csv('India_Air_Quality_Final_Processed.csv', index=False)\n",
        "\n",
        "\n",
        "# VISUAL SUCCESS DASHBOARD (Dark Theme)\n",
        "\n",
        "import plotly.graph_objects as go # Import Plotly for interactive charts\n",
        "\n",
        "# Initialize the figure with a simple \"Number Indicator\"\n",
        "fig = go.Figure(go.Indicator(\n",
        "    mode=\"number\",          # Display only the big numerical value\n",
        "    value=100,              # Set value to 100% to represent completion\n",
        "\n",
        "    # Configure the big number styling\n",
        "    number={'suffix': \"%\", 'font': {'size': 90}},\n",
        "\n",
        "    # Add a title with a subtitle using HTML styling for colors and font sizes\n",
        "    title={'text': \"<b>Preprocessing Complete</b><br><span style='font-size:0.9em;color:#94a3b8'>Scaling ‚Ä¢ Encoding ‚Ä¢ Cleaning ‚Ä¢ Saved</span>\"}\n",
        "))\n",
        "\n",
        "# Configure the overall layout for a sleek, dark appearance\n",
        "fig.update_layout(\n",
        "    height=320,                     # Set figure height\n",
        "    paper_bgcolor=\"#0f172a\",        # Dark blue/slate background\n",
        "    font={'color': \"white\", 'family': \"Arial\"}, # White text for contrast\n",
        "    margin=dict(t=100, b=40, l=40, r=40) # Add top margin for the title\n",
        ")\n",
        "\n",
        "# Add a summary annotation at the bottom\n",
        "# This dynamically displays the number of features scaled and total row count\n",
        "fig.add_annotation(\n",
        "    text=f\"<b>{len(pollutants)}</b> features scaled ‚Ä¢ <b>{df_scaled.shape[0]:,}</b> rows ‚Ä¢ File saved\",\n",
        "    xref=\"paper\", yref=\"paper\",     # Use relative coordinates (0-1)\n",
        "    x=0.5, y=-0.15,                 # Position: Center X, below chart Y\n",
        "    showarrow=False,                # No arrow\n",
        "    font=dict(size=15),             # Font size\n",
        "    bgcolor=\"#1e40af\",              # Blue background box for emphasis\n",
        "    borderpad=12                    # Padding inside the box\n",
        ")\n",
        "\n",
        "# Display the final dashboard\n",
        "fig.show()\n",
        "\n",
        "# Print confirmation to console\n",
        "print(\"Final dataset ready: India_Air_Quality_Final_Processed.csv\")"
      ],
      "metadata": {
        "id": "YaMabqasCJgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I have a new DataFrame, `df_scaled`, where all available pollutant columns are standardized. The fully processed dataset is saved to `India_Air_Quality_Final_Processed.csv`. The original `df` remains unchanged, but now I have a model-ready file with scaled features, a cleaned structure, and a recorded row count. It is ready for consistent use across modeling experiments and the GUI application."
      ],
      "metadata": {
        "id": "yLUb8fkzbgAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4.10 BEFORE VS AFTER PREPROCESSING**\n",
        "I need to visually compare datasets to confirm ML readiness. This process validates feature expansion with total cells, proves that null values are eliminated, ensures that scaling maintains distribution shapes with PM2.5 histograms, and verifies structural coherence using 3D PCA. These checks offer clear visual proof that preprocessing improved data quality without changing the underlying patterns."
      ],
      "metadata": {
        "id": "MtwQJAPNDo8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PCA from scikit-learn for dimensionality reduction (the 3D plot)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. SETUP DATASETS FOR COMPARISON\n",
        "\n",
        "# Create a copy of the raw 'merged_df' to represent the \"Before\" state\n",
        "# We use .copy() to ensure we don't accidentally modify the original dataframe in memory\n",
        "df_before = merged_df.copy()\n",
        "\n",
        "# Create a copy of the processed 'df_scaled' to represent the \"After\" state\n",
        "# This dataset has already gone through cleaning, interpolation, and scaling\n",
        "df_after = df_scaled.copy()\n",
        "\n",
        "\n",
        "# VISUALIZATION 1: DATASET EXPANSION (Total Data Points)\n",
        "\n",
        "# Calculate total data points in the \"Before\" dataset (Rows * Columns)\n",
        "total_cells_before = df_before.shape[0] * df_before.shape[1]\n",
        "# Calculate total data points in the \"After\" dataset\n",
        "# This number should be higher because we added engineered features (Year, Season, etc.)\n",
        "total_cells_after = df_after.shape[0] * df_after.shape[1]\n",
        "\n",
        "# Create a Bar Chart using Plotly Express to visualize the size increase\n",
        "fig_shape = px.bar(\n",
        "    x=['Before', 'After'], # Define the categories for the X-axis\n",
        "    y=[total_cells_before, total_cells_after], # Define the values (counts) for the Y-axis\n",
        "    color=['Before', 'After'], # Color the bars based on their category\n",
        "    color_discrete_map={'Before': '#ef4444', 'After': '#10b981'}, # Set specific colors: Red for Before, Green for After\n",
        "    title=\"<b>1. Dataset Expansion</b> (Rows √ó Cols)\", # Set a bold title for the chart\n",
        "    text=[f\"{df_before.shape[0]:,} rows √ó {df_before.shape[1]} cols\",\n",
        "          f\"{df_after.shape[0]:,} rows √ó {df_after.shape[1]} cols\"] # Define text labels showing dimensions on the bars\n",
        ")\n",
        "\n",
        "# Update the visual traces (bars) to position the text labels outside/above the bars\n",
        "fig_shape.update_traces(textposition='outside')\n",
        "# Configure the overall layout: set height, background colors, and font style\n",
        "fig_shape.update_layout(height=700, paper_bgcolor=\"#0f172a\", plot_bgcolor=\"#0f172a\",\n",
        "                        font_color=\"white\", showlegend=False)\n",
        "# Display the first chart\n",
        "fig_shape.show()\n",
        "\n",
        "\n",
        "# VISUALIZATION 2: MISSING VALUES REDUCTION\n",
        "\n",
        "# Calculate the sum of ALL null values (NaNs) in the \"Before\" dataset\n",
        "missing_before = df_before.isnull().sum().sum()\n",
        "# Calculate the sum of null values in the \"After\" dataset\n",
        "# We select only numeric columns because those are the ones we interpolated and scaled\n",
        "missing_after = df_after.select_dtypes('number').isnull().sum().sum()\n",
        "\n",
        "# Create a Figure using Plotly Graph Objects (allows manual bar creation)\n",
        "fig_missing = go.Figure(data=[\n",
        "    # Create the first bar for \"Before\" counts (Red)\n",
        "    go.Bar(name='Before', x=['Missing Values'], y=[missing_before], marker_color='#ef4444'),\n",
        "    # Create the second bar for \"After\" counts (Green) - ideally this is 0\n",
        "    go.Bar(name='After', x=['Missing Values'], y=[missing_after], marker_color='#10b981')\n",
        "])\n",
        "\n",
        "# Update the layout for the missing values chart\n",
        "fig_missing.update_layout(\n",
        "    title=\"<b>2. Missing Values Eliminated</b>\", # Set title\n",
        "    height=700, # Set height\n",
        "    paper_bgcolor=\"#0f172a\", # Set dark background color for the figure\n",
        "    plot_bgcolor=\"#0f172a\", # Set dark background color for the plot area\n",
        "    font_color=\"white\" # Set text color to white\n",
        ")\n",
        "# Display the second chart\n",
        "fig_missing.show()\n",
        "\n",
        "\n",
        "# VISUALIZATION 3: DISTRIBUTION CHANGE (Side-by-Side Histograms)\n",
        "\n",
        "# Define the specific column (pollutant) we want to inspect\n",
        "col = 'pm25'\n",
        "\n",
        "# Create a subplot figure with 1 row and 2 columns\n",
        "# subplot_titles sets the headings for the left and right graphs\n",
        "fig_hist = make_subplots(rows=1, cols=2, subplot_titles=(f\"Before: Raw {col.upper()}\", f\"After: Scaled {col.upper()}\"))\n",
        "\n",
        "# Add the Histogram trace for the \"Before\" (Raw) data to the first column (1,1)\n",
        "# nbinsx=50 divides the data into 50 bins\n",
        "fig_hist.add_trace(go.Histogram(x=df_before[col], nbinsx=50, name='Raw', marker_color='#ef4444'), row=1, col=1)\n",
        "\n",
        "# Add the Histogram trace for the \"After\" (Scaled) data to the second column (1,2)\n",
        "# Note how the values on the X-axis will be much smaller (centered around 0)\n",
        "fig_hist.add_trace(go.Histogram(x=df_after[col], nbinsx=50, name='Scaled', marker_color='#10b981'), row=1, col=2)\n",
        "\n",
        "# Update the layout for the distribution chart\n",
        "fig_hist.update_layout(\n",
        "    title=f\"<b>3. Distribution Transformation: {col.upper()}</b>\", # Set main title\n",
        "    height=400, # Set height\n",
        "    paper_bgcolor=\"#0f172a\", # Dark background\n",
        "    plot_bgcolor=\"#0f172a\", # Dark plot area\n",
        "    font_color=\"white\", # White text\n",
        "    showlegend=False # Hide the legend to reduce clutter\n",
        ")\n",
        "# Display the third chart\n",
        "fig_hist.show()\n",
        "\n",
        "\n",
        "# VISUALIZATION 4: 3D PCA ANALYSIS (ML Readiness Check)\n",
        "\n",
        "# Define the list of 12 pollutant features to include in the analysis\n",
        "features = ['pm25','pm10','no','no2','nox','nh3','co','so2','o3','benzene','toluene','xylene']\n",
        "\n",
        "# Filter the list to include only columns that actually exist in 'df_after' (safety check)\n",
        "features = [f for f in features if f in df_after.columns]\n",
        "# Select the data for these features\n",
        "X = df_after[features]\n",
        "\n",
        "# --- CRITICAL FIX FOR PCA ---\n",
        "# PCA cannot handle NaNs. Even though we interpolated, some columns might be fully empty.\n",
        "# We fill any remaining NaNs with 0. Since data is scaled (mean=0), 0 represents the average.\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Initialize the PCA algorithm to reduce data to 3 components (dimensions)\n",
        "pca = PCA(n_components=3)\n",
        "# Fit the PCA model to the data X and transform X into the new 3 coordinates\n",
        "components = pca.fit_transform(X)\n",
        "\n",
        "# Create a new DataFrame 'df_pca' to hold the 3D coordinates (PC1, PC2, PC3)\n",
        "df_pca = pd.DataFrame(components, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# Add a 'Category' column for coloring the points\n",
        "# We check if 'aqi_bucket' exists; if so, we use it to color-code points by severity\n",
        "if 'aqi_bucket' in df_after.columns:\n",
        "    # Fill any missing category labels with 'Unknown' to prevent plot errors\n",
        "    df_pca['Category'] = df_after['aqi_bucket'].fillna('Unknown').values\n",
        "else:\n",
        "    # If no bucket column exists, label everything as 'Data Point'\n",
        "    df_pca['Category'] = 'Data Point'\n",
        "\n",
        "# Create the 3D Scatter Plot using Plotly Express\n",
        "fig_pca = px.scatter_3d(\n",
        "    df_pca, # Use the PCA dataframe\n",
        "    x='PC1', y='PC2', z='PC3', # Map the 3 components to X, Y, Z axes\n",
        "    color='Category', # Color points based on the AQI Category\n",
        "    color_discrete_sequence=px.colors.qualitative.Bold, # Use a bold color palette\n",
        "    opacity=0.6, # Make points slightly transparent to see density\n",
        "    size_max=5 # Limit the maximum size of the dots\n",
        ")\n",
        "\n",
        "# Update the layout for the 3D chart\n",
        "fig_pca.update_layout(\n",
        "    title=\"<b>4. Final Quality Check (3D PCA)</b><br><span style='font-size:0.9em;color:#94a3b8'>Visualizing Data Clusters</span>\", # Title\n",
        "    paper_bgcolor=\"#0f172a\", # Dark background\n",
        "    font=dict(color=\"white\", family=\"Arial\"), # Font settings\n",
        "    height=600, # Make the chart taller for better 3D interaction\n",
        "    scene=dict( # Configure the 3D scene (the cube)\n",
        "        xaxis=dict(title='PC1', backgroundcolor=\"#1e293b\", gridcolor=\"#334155\"), # X-axis style\n",
        "        yaxis=dict(title='PC2', backgroundcolor=\"#1e293b\", gridcolor=\"#334155\"), # Y-axis style\n",
        "        zaxis=dict(title='PC3', backgroundcolor=\"#1e293b\", gridcolor=\"#334155\"), # Z-axis style\n",
        "    ),\n",
        "    margin=dict(t=80, b=20, l=0, r=0) # Set margins\n",
        ")\n",
        "# Display the final 3D chart\n",
        "fig_pca.show()"
      ],
      "metadata": {
        "id": "2GT7DfYFGrU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "I gain four important artifacts: evidence of dataset growth, confirmation that there are no missing values, checks comparing raw and scaled distributions, and a PCA cluster analysis. While the data itself remains unchanged, the final dataset (`df_after`) is now confirmed as a strong modeling input, supported by clear proof of improved structure and completeness."
      ],
      "metadata": {
        "id": "8_3Lgsz4cHJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C8ycjaVAuv9"
      },
      "source": [
        "#**2. Task 2: Data Analysis (EDA)**\n",
        "\n",
        "Now that my data is clean, I need to understand its underlying structure. Exploratory Data Analysis (EDA) is not about modeling; it is about \"listening\" to the data to detect patterns, anomalies, and relationships that will dictate my modeling strategy.\n",
        "\n",
        "In this step, I am using statistical visualization to answer three key questions:\n",
        "1.  **How are variables related?** (Correlation Matrix) - This tells me if feature A affects feature B, helping me avoid multicollinearity in regression models.\n",
        "2.  **How is the data distributed?** (Histograms) - This reveals if my data follows a normal distribution (bell curve) or if it is skewed, which determines which statistical tests I can use.\n",
        "3.  **How does it change over time?** (Time Series) - If temporal data exists, I need to see the trends and seasonality.\n",
        "\n",
        "As Tukey (1977) famously stated, the greatest value of a picture is when it forces us to notice what we never expected to see.\n",
        "\n",
        "**References:**\n",
        "\n",
        "i) Tukey, J.W. (1977) *Exploratory Data Analysis*. Reading, MA: Addison-Wesley. Available at: https://books.google.com/books/about/Exploratory_Data_Analysis.html?id=UT9dAAAAIAAJ (Accessed: 4 December 2025).\n",
        "\n",
        "ii) Wickham, H. (2016) *ggplot2: Elegant Graphics for Data Analysis*. New York: Springer-Verlag. Available at: https://link.springer.com/book/10.1007/978-3-319-24277-4 (Accessed: 4 December 2025)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.1 CALENDAR HEATMAP (The \"Pollution Calendar\")**\n",
        "I need to build this year, month AQI heatmap to show the seasonal structure in air quality that simple line plots or global averages would miss. By using a Year √ó Month matrix of mean AQI, I can quickly see when the air quality is consistently poor across multiple years. This is crucial for a forecasting project. If winter months (November to February) consistently show higher AQI, my models and policy recommendations must take that seasonality into account instead of treating all months as the same. Using clear month labels and a red color scale makes the pattern easy to understand. Darker red signals hazardous periods, so stakeholders can quickly find \"danger seasons\" without needing to read through detailed tables. The annotation highlighting the winter peak transforms a technical plot into a clear insight. It connects the visual pattern to a specific statement about when air quality is most concerning. This is exactly the type of story the assessment expects from exploratory data analysis."
      ],
      "metadata": {
        "id": "0wzshdXsKoxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARE THE DATA (PIVOTING)\n",
        "# We need to transform the data into a \"Matrix\" format (Grid) where:\n",
        "# Rows = Years, Columns = Months, Values = Average AQI\n",
        "pivot = df.pivot_table(\n",
        "    values='aqi',    # The data we want to aggregate (Air Quality Index)\n",
        "    index='year',    # The rows of the heatmap\n",
        "    columns='month', # The columns of the heatmap\n",
        "    aggfunc='mean'   # Calculate the Average AQI for that specific Month/Year\n",
        ").round(0)           # Round to nearest whole number for cleaner visualization\n",
        "\n",
        "# BEAUTIFY LABELS\n",
        "# Create a list of proper month names to replace numbers (1, 2, 3...)\n",
        "month_names = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "# Assign these names to the pivot table columns so the X-axis shows \"Jan\" instead of \"1\"\n",
        "pivot.columns = month_names\n",
        "\n",
        "# INITIALIZE THE HEATMAP\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=pivot.values,      # The 'Z' axis determines the Color intensity (AQI values)\n",
        "    x=pivot.columns,     # The 'X' axis labels (Jan, Feb, Mar...)\n",
        "    y=pivot.index,       # The 'Y' axis labels (2015, 2016, 2017...)\n",
        "    colorscale='Reds',   # Use a Red color scale (intuitive: Darker Red = Higher Pollution/Danger)\n",
        "\n",
        "    # Text Configuration (Showing numbers inside the squares)\n",
        "    text=pivot.values,   # The text to display is the AQI value itself\n",
        "    texttemplate=\"%{text}\", # Standard format for displaying the text\n",
        "    textfont={\"size\":14, \"color\":\"white\"}, # White text pops against red background\n",
        "\n",
        "    # Hover Configuration (Interactive Tooltip)\n",
        "    hoverongaps=False,   # Do not show hover info on empty data points\n",
        "    # Custom HTML tooltip: Shows Year, Month, and specific AQI value clearly\n",
        "    hovertemplate=\"<b>Year:</b> %{y}<br><b>Month:</b> %{x}<br><b>Avg AQI:</b> %{z}<extra></extra>\",\n",
        "\n",
        "    # Color Bar Configuration (The Legend on the right)\n",
        "    colorbar=dict(title=\"Mean AQI\", thickness=20)\n",
        "))\n",
        "\n",
        "# LAYOUT STYLING\n",
        "fig.update_layout(\n",
        "    # Title Configuration with Subtitle\n",
        "    title={\n",
        "        'text': \"<b>Seasonal AQI Trends: Year vs Month Heatmap</b><br><sub>Highest pollution in winter months (Nov‚ÄìFeb)</sub>\",\n",
        "        'x': 0.5, # Center the title horizontally\n",
        "        'font': dict(size=22, family=\"Arial Black\", color=\"#1e293b\") # Professional dark font\n",
        "    },\n",
        "    paper_bgcolor=\"#f8fafc\", # Light grey background for the outer area\n",
        "    plot_bgcolor=\"white\",    # White background for the plot area\n",
        "    height=600,              # Set a comfortable height\n",
        "\n",
        "    # Axis Styling\n",
        "    xaxis=dict(title=\"<b>Month</b>\", tickfont=dict(size=14)), # Bold Label for X-axis\n",
        "    yaxis=dict(title=\"<b>Year</b>\", tickfont=dict(size=14)),  # Bold Label for Y-axis\n",
        "    font=dict(family=\"Arial\", size=14) # Global font setting\n",
        ")\n",
        "\n",
        "# DATA STORYTELLING (ANNOTATION)\n",
        "# Add an arrow and text to highlight the specific area of interest (Winter)\n",
        "fig.add_annotation(\n",
        "    text=\"Winter Peak<br>(Nov‚ÄìFeb)\", # The insight we want to draw attention to\n",
        "    x=\"Dec\",           # Point the arrow at December (X-axis)\n",
        "    y=pivot.index[-1], # Point the arrow at the most recent year (Y-axis)\n",
        "    xref=\"x\", yref=\"y\", # Use data coordinates\n",
        "\n",
        "    # Arrow Styling\n",
        "    showarrow=True,\n",
        "    arrowhead=2,\n",
        "    arrowsize=1,\n",
        "    arrowwidth=3,\n",
        "    arrowcolor=\"#dc2626\", # Bright red arrow\n",
        "\n",
        "    # Text Box Positioning (Offset from the arrow tip)\n",
        "    ax=60, ay=-40,\n",
        "\n",
        "    # Text Box Styling\n",
        "    font=dict(size=14, color=\"#dc2626\"),\n",
        "    bgcolor=\"rgba(220,38,38,0.1)\", # Semi-transparent red background\n",
        "    bordercolor=\"#dc2626\",\n",
        "    borderwidth=2\n",
        ")\n",
        "\n",
        "# DISPLAY CHART\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7XIMb7xXKrhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I have a pivoted table summarizing mean AQI by year and month, along with a polished heatmap that clearly shows seasonal pollution peaks, especially in winter months. The underlying data frame remains the same, but my understanding‚Äîand the reader's‚Äînow includes a clear, visually supported insight into how AQI changes throughout the year. This will guide my later modeling choices, such as including seasonal features, and shape the narrative in my report."
      ],
      "metadata": {
        "id": "6pEQR7xkcqX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.2 ROLLING AVERAGES (Trend vs Noise)**\n",
        "I need to separate short-term \"noise\" from the main AQI trend. This helps me understand real seasonal behavior without overreacting to daily spikes. By sorting the data by date and calculating a 30-day rolling average, I get a smoother signal that shows broader cycles and changes, unlike raw daily data. For a forecasting and policy-support project, it‚Äôs crucial to demonstrate how AQI changes over months, not just on a few bad days. Plotting the daily data alongside the 30-day trend allows me to maintain detail while clearly showing the overall pattern. The shaded winter area and annotations convert this from a simple time series into a clear insight: winter from 2019 to 2020 is consistently the worst period. Without this type of trend analysis, I would find it difficult to explain seasonal attributes, interpret model results over time, or clearly convey risk periods to non-technical stakeholders."
      ],
      "metadata": {
        "id": "LWAl9NngK6ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARE THE DATA\n",
        "# Sort by date to ensure the rolling average calculates correctly over time.\n",
        "# We work with a copy to avoid SettingWithCopy warnings.\n",
        "df_sorted = df.sort_values('date').copy()\n",
        "\n",
        "\n",
        "# CALCULATE THE TREND (SIGNAL)\n",
        "# .rolling(window=30): Looks at the \"last 30 days\" window for each row.\n",
        "# .mean(): Calculates the average of that window.\n",
        "# This smooths out daily spikes to show the underlying monthly trend.\n",
        "df_sorted['aqi_30d_avg'] = df_sorted['aqi'].rolling(window=30, min_periods=1).mean()\n",
        "\n",
        "# INITIALIZE FIGURE\n",
        "fig = go.Figure()\n",
        "\n",
        "# LAYER 1: THE NOISE (Daily Raw Data)\n",
        "# I plot this first so it sits in the background.\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_sorted['date'],\n",
        "    y=df_sorted['aqi'],\n",
        "    mode='lines',\n",
        "    name='Daily AQI', # Legend label\n",
        "    # Style: Thin line, light blue, and semi-transparent (rgba) to minimize visual clutter\n",
        "    line=dict(color='rgba(100, 149, 237, 0.4)', width=1),\n",
        "    # Tooltip: Shows specific date and integer AQI\n",
        "    hovertemplate=\"Date: %{x|%b %Y}<br>AQI: %{y:.0f}<extra></extra>\"\n",
        "))\n",
        "\n",
        "# LAYER 2: THE SIGNAL (30-Day Trend)\n",
        "# I plot this second so it sits ON TOP of the raw data.\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_sorted['date'],\n",
        "    y=df_sorted['aqi_30d_avg'],\n",
        "    mode='lines',\n",
        "    name='30-Day Trend', # Legend label\n",
        "    # Style: Thick line (5px), solid dark blue to capture attention\n",
        "    line=dict(color='#1e40af', width=5),\n",
        "    # Tooltip: Shows the smoothed average with 1 decimal place\n",
        "    hovertemplate=\"Date: %{x|%b %Y}<br>30-Day Avg: %{y:.1f}<extra></extra>\"\n",
        "))\n",
        "\n",
        "# LAYOUT CONFIGURATION\n",
        "fig.update_layout(\n",
        "    # Title with Subtitle for context\n",
        "    title={\n",
        "        'text': \"<b>AQI Trend Analysis</b><br><sub>30-Day Moving Average Reveals Clear Seasonal Cycles</sub>\",\n",
        "        'x': 0.5, # Center alignment\n",
        "        'font': dict(size=22, family=\"Arial Black\") # Professional font\n",
        "    },\n",
        "\n",
        "    # X-Axis Configuration (Time)\n",
        "    xaxis=dict(\n",
        "        title=\"\", # No title needed (Dates are obvious)\n",
        "        tickformat=\"%b %Y\", # Format ticks as \"Jan 2020\"\n",
        "        gridcolor=\"#e2e8f0\", # Subtle grid lines\n",
        "\n",
        "        # Add Interactive Range Selector Buttons (1 Year, 3 Years, All)\n",
        "        rangeselector=dict(\n",
        "            buttons=list([\n",
        "                dict(count=1, label=\"1Y\", step=\"year\", stepmode=\"backward\"),\n",
        "                dict(count=3, label=\"3Y\", step=\"year\", stepmode=\"backward\"),\n",
        "                dict(step=\"all\", label=\"All\")\n",
        "            ])\n",
        "        ),\n",
        "        # Add the Slider at the bottom\n",
        "        rangeslider=dict(visible=True),\n",
        "        type=\"date\"\n",
        "    ),\n",
        "\n",
        "    # Y-Axis Configuration (AQI Level)\n",
        "    yaxis=dict(\n",
        "        title=\"AQI Level\",\n",
        "        gridcolor=\"#e2e8f0\",\n",
        "        # Set range dynamically: 0 to 110% of the max value (adds headroom at top)\n",
        "        range=[0, df_sorted['aqi'].max() * 1.1]\n",
        "    ),\n",
        "\n",
        "    # General Aesthetic\n",
        "    height=600,              # Figure height\n",
        "    paper_bgcolor=\"#f8fafc\", # Light grey outer background\n",
        "    plot_bgcolor=\"white\",    # White inner plot background\n",
        "    hovermode=\"x unified\",   # Show all values for a specific date in one tooltip\n",
        "\n",
        "    # Legend Styling (Top Left, floating)\n",
        "    legend=dict(\n",
        "        x=0.02, y=0.98,\n",
        "        bgcolor=\"rgba(255,255,255,0.8)\", # Semi-transparent white background\n",
        "        bordercolor=\"#cbd5e1\",\n",
        "        borderwidth=1\n",
        "    ),\n",
        "    # Margins to fit titles and sliders\n",
        "    margin=dict(t=120, b=60, l=60, r=60)\n",
        ")\n",
        "\n",
        "# DATA STORYTELLING\n",
        "\n",
        "# Highlight the Winter 2019-2020 Peak Zone\n",
        "# This draws a vertical shaded rectangle over the worst pollution period\n",
        "fig.add_vrect(\n",
        "    x0=\"2019-11-01\", x1=\"2020-02-29\", # Start and End dates\n",
        "    fillcolor=\"#dc2626\", # Red highlight\n",
        "    opacity=0.1, # Very transparent\n",
        "    layer=\"below\", # Place behind the lines\n",
        "    line_width=0, # No border\n",
        "    annotation_text=\"Winter Peak\", # Label\n",
        "    annotation_position=\"top left\",\n",
        "    annotation=dict(font_size=14, font_color=\"#dc2626\", bgcolor=\"white\")\n",
        ")\n",
        "\n",
        "# Add a Text Annotation explaining the insight\n",
        "fig.add_annotation(\n",
        "    text=\"Strong seasonal pattern:<br>Winter (Nov‚ÄìFeb) consistently worst\",\n",
        "    x=\"2020-01-01\", # Position near the 2020 peak\n",
        "    y=df_sorted['aqi_30d_avg'].max() * 0.9,\n",
        "    showarrow=True, # Add an arrow pointing to the data\n",
        "    arrowhead=2, arrowsize=2, arrowcolor=\"#dc2626\",\n",
        "    bgcolor=\"rgba(220,38,38,0.1)\", # Red background box\n",
        "    bordercolor=\"#dc2626\",\n",
        "    font=dict(size=13)\n",
        ")\n",
        "\n",
        "# Display the final chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "cZwYY2llK6h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I added a new column `aqi_30d_avg` in `df_sorted` that captures the 30-day moving average. I also created a detailed time-series chart that overlays raw AQI with the smoothed trend and emphasizes the winter peak. The original `df` is unchanged, but I now have clear visual proof of strong seasonal patterns. This directly supports the use of time-based features and seasonal interpretation in future modeling and reporting."
      ],
      "metadata": {
        "id": "06jR7ypAdnMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.3 MULTI-VARIATE PAIR PLOT (Chemical Interactions)**\n",
        "I need to use a scatter matrix here to show how key pollutants behave together and how strongly they relate to AQI. Simple univariate plots cannot demonstrate this. By plotting PM2.5, PM10, NO‚ÇÇ, SO‚ÇÇ, and AQI against each other and coloring by `aqi_bucket`, I can identify whether high-severity categories cluster in specific regions of pollutant space. For instance, I can see if severe AQI usually aligns with high PM2.5 and PM10 levels or if NO‚ÇÇ remains high even when the overall AQI is moderate. This is essential for answering the question, ‚Äúwhat drives AQI?‚Äù in a multivariate way and for supporting feature importance later in the modeling. Including `date_str` in the hover data allows me to link individual points to time, helping me determine if clusters relate to specific episodes or seasons. Without this step, my understanding of pollutant relationships would mainly rely on anecdotes, and I would lack strong visual evidence that PM2.5 is the main driver, while gases like NO‚ÇÇ behave differently across severity bands."
      ],
      "metadata": {
        "id": "tVn6orC4LH2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARE DATA FOR HOVERING\n",
        "# Create a formatted string column for Date (e.g., \"Jan 2020\")\n",
        "# This makes the tooltip look professional and clean.\n",
        "df['date_str'] = df['date'].dt.strftime('%b %Y')\n",
        "\n",
        "# Define the columns to visualize in the matrix\n",
        "cols = ['pm25', 'pm10', 'no2', 'so2', 'aqi']\n",
        "\n",
        "# GENERATE THE SCATTER MATRIX\n",
        "fig = px.scatter_matrix(\n",
        "    df, # Use the dataframe\n",
        "    dimensions=cols, # The columns to plot against each other\n",
        "    color=\"aqi_bucket\", # Color points by their Severity bucket\n",
        "\n",
        "    # Define a custom semantic color map (Green -> Red)\n",
        "    color_discrete_map={\n",
        "        \"Good\": \"#22c55e\", \"Satisfactory\": \"#3b82f6\", \"Moderate\": \"#f59e0b\",\n",
        "        \"Poor\": \"#f97316\", \"Very Poor\": \"#dc2626\", \"Severe\": \"#7f1d1d\"\n",
        "    },\n",
        "\n",
        "    # CRITICAL FIX: Add extra data for tooltips here (Plotly handles the splitting)\n",
        "    # This adds 'aqi' and 'date_str' to the internal customdata array automatically.\n",
        "    hover_data={'aqi':True, 'date_str':True, 'aqi_bucket':False},\n",
        "\n",
        "    title=\"\", # We will set a custom styled title later\n",
        "    height=900, # Set a large height for detail\n",
        ")\n",
        "\n",
        "# PROFESSIONAL STYLING & INTERACTIVITY\n",
        "fig.update_traces(\n",
        "    diagonal_visible=False, # Hide the diagonal (histograms) to focus on correlations\n",
        "\n",
        "    # Style the markers: smaller, transparent, with a white border for clarity\n",
        "    marker=dict(size=5, opacity=0.7, line=dict(width=0.5, color='white')),\n",
        "\n",
        "    # Custom Hover Template\n",
        "    # %{customdata[1]} refers to 'date_str' (the 2nd item passed in hover_data)\n",
        "    hovertemplate=\"<b>%{xaxis.title.text}</b>: %{x:.1f}<br>\"\n",
        "                  \"<b>%{yaxis.title.text}</b>: %{y:.1f}<br>\"\n",
        "                  \"<b>Date</b>: %{customdata[1]}<extra></extra>\"\n",
        ")\n",
        "\n",
        "# LAYOUT CONFIGURATION\n",
        "fig.update_layout(\n",
        "    # Title with Subtitle (HTML styling)\n",
        "    title={\n",
        "        'text': \"<b>Pollutant Relationships & AQI Severity</b><br>\"\n",
        "                \"<sub>Strong PM2.5‚ÄìPM10 correlation ‚Ä¢ NO‚ÇÇ often high with moderate AQI ‚Ä¢ Hover for details</sub>\",\n",
        "        'x': 0.5, # Center the title\n",
        "        'font': dict(size=22, family=\"Arial Black\", color=\"#1e293b\") # Dark professional font\n",
        "    },\n",
        "    paper_bgcolor=\"#f8fafc\", # Light grey outer background\n",
        "    plot_bgcolor=\"white\",    # White inner background\n",
        "    font=dict(family=\"Arial\", size=11), # Global font size\n",
        "\n",
        "    # Legend Styling (Floating top-right)\n",
        "    legend=dict(title=\"<b>AQI Category</b>\", x=1.02, y=0.98, bgcolor=\"rgba(255,255,255,0.9)\"),\n",
        "\n",
        "    # Margins to accommodate the large title and annotations\n",
        "    margin=dict(t=140, r=100, l=80, b=80)\n",
        ")\n",
        "\n",
        "# DATA STORYTELLING\n",
        "# Highlight the key insight about PM2.5 driving the AQI score\n",
        "fig.add_annotation(\n",
        "    text=\"PM2.5 drives AQI most<br>NO‚ÇÇ often high even at moderate AQI\",\n",
        "    xref=\"paper\", yref=\"paper\", # Relative coordinates\n",
        "    x=0.05, y=0.98, # Position: Top Left\n",
        "    showarrow=True,\n",
        "    arrowhead=2, arrowsize=1.5, arrowcolor=\"#dc2626\", # Red Arrow\n",
        "    font=dict(size=14, color=\"#dc2626\"), # Red Text\n",
        "    bgcolor=\"rgba(220,38,38,0.1)\", # Semi-transparent red box\n",
        "    bordercolor=\"#dc2626\", borderwidth=2, borderpad=10\n",
        ")\n",
        "\n",
        "# Display the final interactive chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ajrpy4_9LIgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, `df` has a `date_str` column for clear date display, and I now have an interactive scatter matrix that displays pairwise pollutant relationships colored by AQI category, complete with helpful hover tooltips and annotations. The underlying data values remain the same, but I now have an effective visual tool to support claims about the correlation between PM2.5 and PM10, NO‚ÇÇ behavior, and how combinations of pollutants relate to AQI severity."
      ],
      "metadata": {
        "id": "pGFIhmqieMlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.4 AUTOCORRELATION / LAG PLOT (Predictability Check)**\n",
        "\n",
        "I have created this lag plot to test whether AQI has strong temporal persistence. This is necessary for using time-series models like ARIMA or LSTM. By plotting today‚Äôs AQI against tomorrow‚Äôs AQI and adding the \\(y = x\\) line, I can observe if the points cluster closely around the diagonal. If they do, it indicates that \"tomorrow is similar to today,\" meaning the series is highly predictable based on its past. Without this check, selecting a time-series forecasting approach would just be an assumption rather than a well-supported choice. Calculating the Pearson correlation between \\(AQI_t\\) and \\(AQI_{t+1}\\) measures that persistence. A very high correlation value suggests that past AQI provides significant information about future AQI. This directly influences my modeling decisions and helps me explain in the report why autoregressive methods are suitable for this dataset."
      ],
      "metadata": {
        "id": "YuLjw7epLTNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty figure canvas\n",
        "fig = go.Figure()\n",
        "\n",
        "# MAIN SCATTER TRACE: TODAY vs TOMORROW\n",
        "\n",
        "# This creates the dots. If they form a tight diagonal line, the data is predictable.\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df['aqi'],          # X-axis: \"AQI at time t\" (Today's value)\n",
        "    y=df['aqi'].shift(-2), # Y-axis: \"AQI at time t+1\" (Tomorrow's value). shift(-1) moves data UP 1 row.\n",
        "    mode='markers',       # Use dots (markers) instead of connecting lines\n",
        "\n",
        "    # Marker Styling: Make the dots look professional and informative\n",
        "    marker=dict(\n",
        "        size=7,           # Set dot size to 7 pixels\n",
        "        color=df['aqi'],  # Color the dots based on AQI value (Darker = Higher Pollution)\n",
        "        colorscale='Reds',# Use the 'Reds' color palette\n",
        "        opacity=0.7,      # Make dots semi-transparent to show density where points overlap\n",
        "        line=dict(width=0.8, color='white'), # Add a thin white border to each dot for pop\n",
        "        colorbar=dict(title=\"AQI Level\", thickness=10) # Add a legend bar on the right\n",
        "    ),\n",
        "\n",
        "    name=\"AQI(t) vs AQI(t+1)\", # Name for the legend (if visible)\n",
        "    hovertemplate=\"<b>Today:</b> %{x:.0f}<br><b>Tomorrow:</b> %{y:.0f}<extra></extra>\" # Custom tooltip format\n",
        "))\n",
        "\n",
        "# REFERENCE LINE: PERFECT PERSISTENCE (y = x)\n",
        "\n",
        "# If a dot falls exactly on this line, it means Today's AQI is IDENTICAL to Tomorrow's.\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[df['aqi'].min(), df['aqi'].max()], # Start and End X coordinates (Full range)\n",
        "    y=[df['aqi'].min(), df['aqi'].max()], # Start and End Y coordinates (Same as X for diagonal)\n",
        "    mode='lines',       # Draw a line, not dots\n",
        "    line=dict(color=\"#dc2626\", width=4, dash='dash'), # Style: Thick, red, dashed line\n",
        "    name=\"Perfect Persistence (y = x)\"\n",
        "))\n",
        "\n",
        "# LAYOUT CONFIGURATION\n",
        "\n",
        "fig.update_layout(\n",
        "    # Title with Subtitle for context\n",
        "    title={\n",
        "        'text': \"<b>Lag Plot Analysis</b><br>\"\n",
        "                \"<sub>Strong clustering along diagonal ‚Üí High autocorrelation ‚Üí Tomorrow ‚âà Today</sub>\",\n",
        "        'x': 0.5, # Center the title horizontally\n",
        "        'font': dict(size=22, family=\"Arial Black\", color=\"#1e293b\") # Bold, dark font\n",
        "    },\n",
        "    # This moves \"AQI(t) vs AQI(t+1)\" and \"Perfect Persistence\"\n",
        "    # away from the \"AQI Level\" colorbar at the top right.\n",
        "    legend=dict(\n",
        "        x=0.7,           # Move to the right side (0 is left, 1 is right)\n",
        "        y=0.05,          # Move to the bottom (0 is bottom, 1 is top)\n",
        "        bgcolor=\"rgba(255,255,255,0.8)\", # Semi-transparent background\n",
        "        bordercolor=\"#e2e8f0\",\n",
        "        borderwidth=1\n",
        "    ),\n",
        "    # Axis Configurations\n",
        "    xaxis=dict(title=\"<b>AQI Today</b>\", range=[0, df['aqi'].max()*1.05], gridcolor=\"#e2e8f0\"),\n",
        "    yaxis=dict(title=\"<b>AQI Tomorrow</b>\", range=[0, df['aqi'].max()*1.05], gridcolor=\"#e2e8f0\"),\n",
        "\n",
        "    # General Aesthetic Settings\n",
        "    height=600,              # Figure height\n",
        "    paper_bgcolor=\"#f8fafc\", # Light grey outer background\n",
        "    plot_bgcolor=\"white\",    # White inner plot background\n",
        "    hovermode=\"closest\",     # Tooltip highlights the specific point you hover over\n",
        "    font=dict(family=\"Arial\", size=13), # Global font settings\n",
        "    margin=dict(t=130, b=80, l=80, r=80) # Margins to accommodate titles and labels\n",
        ")\n",
        "\n",
        "\n",
        "# INSIGHT (Why does this matter?)\n",
        "\n",
        "# Adds a box explaining that this pattern validates using ARIMA/LSTM models\n",
        "fig.add_annotation(\n",
        "    text=\"Very Strong Persistence<br>‚Üí Excellent candidate for time-series forecasting<br>(ARIMA, LSTM, Prophet)\",\n",
        "    x=df['aqi'].median(), # Position text near the median (center of the cloud)\n",
        "    y=df['aqi'].max() * 0.6, # Position text near the top of the chart\n",
        "    showarrow=True,      # Draw an arrow pointing to the data\n",
        "    arrowhead=2, arrowsize=2, arrowcolor=\"#dc2626\", # Arrow styling\n",
        "    font=dict(size=15, color=\"#171616\"), # Text styling\n",
        "    bgcolor=\"rgba(220,38,38,0.1)\", # Semi-transparent red background box\n",
        "    bordercolor=\"#dc2626\", # Red border\n",
        "    borderwidth=3, borderpad=12 # Border thickness and padding\n",
        ")\n",
        "\n",
        "\n",
        "# STATISTICAL CORRELATION ANNOTATION\n",
        "\n",
        "# Calculate the actual Pearson Correlation coefficient\n",
        "corr = df['aqi'].corr(df['aqi'].shift(-1))\n",
        "\n",
        "# Display the correlation value in the top-left corner\n",
        "fig.add_annotation(\n",
        "    text=f\"Correlation = <b>{corr:.3f}</b><br>(>0.9 = Extremely Predictable)\", # Formatted string\n",
        "    xref=\"paper\", yref=\"paper\", # Use relative coordinates (0-1)\n",
        "    x=0.02, y=0.98, # Position: Top-Left\n",
        "    showarrow=False, # No arrow needed\n",
        "    font=dict(size=14, color=\"#16a34a\"), # Green text\n",
        "    bgcolor=\"#ecfdf5\", # Light green background\n",
        "    bordercolor=\"#16a34a\", # Green border\n",
        "    borderwidth=2, borderpad=10\n",
        ")\n",
        "\n",
        "# Render the interactive chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "l50FCPR1LTqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, the underlying data remains unchanged. However, I now have a lag-scatter plot, a visual persistence line, an interpretive annotation, and a numeric correlation value between today‚Äôs and tomorrow‚Äôs AQI. Together, these elements demonstrate that AQI has strong day-to-day autocorrelation. This gives me solid evidence to treat AQI as a predictable time series and to justify using forecasting models that rely on past values as inputs."
      ],
      "metadata": {
        "id": "eGugrFkWezOg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1sUPf-mEH4X"
      },
      "source": [
        "#**3. Task 3: Model Building**\n",
        "\n",
        "This is the core of the project. Having cleaned and explored the data, I am now entering the **Modeling** phase of the CRISP-DM lifecycle. My goal is to train algorithms to learn the underlying patterns in the data so they can predict future outcomes.\n",
        "\n",
        "I am adopting a \"Tournament\" strategy here. Instead of relying on a single algorithm, I will train three distinct types of models to see which fits the data best:\n",
        "1.  **Linear Regression:** A baseline model that assumes linear relationships (good for simplicity).\n",
        "2.  **Random Forest:** An ensemble of decision trees that captures complex, non-linear patterns (Breiman, 2001).\n",
        "3.  **XGBoost:** A gradient boosting machine that corrects previous errors, often winning Kaggle competitions for tabular data.\n",
        "\n",
        "I will split the data into **Training (80%)** and **Testing (20%)** sets. This ensures that I evaluate the models on data they have never seen before, protecting against overfitting (Hastie, Tibshirani and Friedman, 2009).\n",
        "\n",
        "**References:**\n",
        "\n",
        "i) Breiman, L. (2001) ‚ÄòRandom Forests‚Äô, *Machine Learning*, 45(1), pp. 5‚Äì32. Available at: https://link.springer.com/article/10.1023/A:1010933404324 (Accessed: 4 December 2025).\n",
        "\n",
        "ii) Hastie, T., Tibshirani, R. and Friedman, J. (2009) *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. 2nd edn. New York: Springer. Available at: https://link.springer.com/book/10.1007/978-0-387-84858-7 (Accessed: 4 December 2025)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.1 DEFINE FEATURES (X) AND TARGET (y)**\n",
        "I need to clearly separate features (X) from the target (y) to avoid data leakage and to provide the model only with information it would realistically have when making predictions. Dropping `aqi` and both AQI-derived columns (`aqi_bucket`, `aqi_bucket_encoded`) from X is crucial. Including them would allow the model to \"cheat\" by learning from the answer or its direct transformations. Removing `date` and raw `city` ensures that X contains only numeric, model-ready inputs, relying instead on already created temporal features (year, month, season, etc.). This approach makes my feature set cleaner, prevents errors with non-numeric data types, and keeps the baseline model conceptually honest. The diagram clarifies for both the reader and me that X consists of \"pollutants + engineered features,\" while y is \"AQI only.\" This is the exact structure needed for a well-designed supervised learning setup."
      ],
      "metadata": {
        "id": "ruYixmo_PLZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a console message to track progress\n",
        "print(\"Defining Features (X) and Target (y)\")\n",
        "\n",
        "# DEFINE FEATURES (X) AND TARGET (y)\n",
        "# Define X (Features/Inputs):\n",
        "# We drop columns that CANNOT be used for prediction:\n",
        "# - 'aqi': This is the answer (target), so we remove it.\n",
        "# - 'aqi_bucket': This is text derived from AQI, so it's a form of cheating (leakage).\n",
        "# - 'date': Machine learning models need numbers, not datetime objects (we already extracted Year/Month/etc).\n",
        "# - 'city': Text column (unless one-hot encoded, it's safer to drop for a baseline model).\n",
        "# - 'aqi_bucket_encoded': Also derived from AQI, so it's leakage.\n",
        "# errors='ignore' ensures the code doesn't crash if one of these columns is already missing.\n",
        "X = df.drop(columns=['aqi', 'aqi_bucket', 'date', 'city', 'aqi_bucket_encoded'], errors='ignore')\n",
        "\n",
        "# Define y (Target/Output):\n",
        "# This is the single column we are trying to predict.\n",
        "y = df['aqi']\n",
        "\n",
        "# VISUAL CONFIRMATION DASHBOARD\n",
        "\n",
        "# Initialize an empty Plotly figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add two large circles to represent \"Features (X)\" and \"Target (y)\"\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[0, 1],       # X-coordinates for the two circles (Left and Right)\n",
        "    y=[1, 1],       # Y-coordinates (Same height)\n",
        "    mode=\"markers+text\", # Show both the circle shape and text inside\n",
        "    # Marker styling: Large size (120px), distinct colors (Blue for X, Green for y)\n",
        "    marker=dict(size=120, color=[\"#3b82f6\", \"#10b981\"], symbol=\"circle\"),\n",
        "    # Text labels to display inside the circles\n",
        "    text=[\"X<br>Features\", \"y<br>AQI Target\"],\n",
        "    textposition=\"middle center\", # Center the text\n",
        "    textfont=dict(size=18, color=\"white\"), # White, readable font\n",
        "    hoverinfo=\"none\" # Disable hover tooltip (not needed for this static diagram)\n",
        "))\n",
        "\n",
        "# Add a dashed vertical line in the middle to visually separate Inputs vs Output\n",
        "fig.add_shape(\n",
        "    type=\"line\",\n",
        "    x0=0.5, y0=0.8, x1=0.5, y1=1.2, # Coordinates for a short vertical line between the circles\n",
        "    line=dict(color=\"#64748b\", width=4, dash=\"dot\") # Grey, thick, dotted style\n",
        ")\n",
        "\n",
        "# Configure the overall layout\n",
        "fig.update_layout(\n",
        "    # Title with Subtitle (HTML styling)\n",
        "    title=\"<b>Features & Target Defined</b><br><sub>X ‚Üí Pollutants & Engineered Features | y ‚Üí AQI</sub>\",\n",
        "    title_x=0.5,            # Center the title\n",
        "    paper_bgcolor=\"#bee8d7\", # Dark blue background\n",
        "    plot_bgcolor=\"#39173b\",  # Dark plot area\n",
        "    height=300,             # Figure height\n",
        "    showlegend=False,       # Hide legend\n",
        "    # Hide all axis elements (grids, numbers) for a clean \"diagram\" look\n",
        "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "    margin=dict(t=100, b=20, l=40, r=40) # Margins\n",
        ")\n",
        "\n",
        "# Add a text annotation at the bottom summary stats\n",
        "fig.add_annotation(\n",
        "    # Dynamically display the count of features (columns in X) and samples (rows in X)\n",
        "    text=f\"<b>{X.shape[1]}</b> Features<br><b>{X.shape[0]:,}</b> Samples\",\n",
        "    x=0.5, y=0.3, # Position: Bottom Center\n",
        "    showarrow=False, # No arrow needed\n",
        "    font=dict(size=16, color=\"#e2e8f0\") # Light grey text\n",
        ")\n",
        "\n",
        "# Display the interactive diagram\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "NKkeDHo_PIC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I have two clear objects: `X`, which contains only the selected predictor variables, and `y`, which holds the AQI values to predict. The Plotly diagram shows how many features and samples are in X. The underlying DataFrame `df` remains unchanged, but the modeling stage now has a clean, leakage-free feature matrix and a clearly defined target, ready for splitting into training and testing sets, as well as for model training."
      ],
      "metadata": {
        "id": "yyeOrkHkfnzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.2 SPLIT DATA (TRAIN vs TEST)**\n",
        "I need to divide the data into separate training and test sets. This way, I can see how well my models work with new data instead of just memorizing the examples they were trained on. Using an 80/20 split is a common choice. It provides enough data to learn strong patterns and enough data held back to give a real evaluation. If I trained and tested on the same dataset, the performance metrics would be too positive and not credible. Setting `random_state=42` is important. It makes the split reproducible, so others can exactly replicate my results, comparisons between models, and reported metrics. The donut chart and annotation help turn this technical step into a clear visual of how many samples went into training versus testing. This adds credibility to my later model evaluation."
      ],
      "metadata": {
        "id": "unbztY2PPZY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a status message to the console\n",
        "print(\"Splitting Dataset: 80% Train, 20% Test\")\n",
        "\n",
        "# PERFORM THE SPLIT\n",
        "# Use the train_test_split function from Scikit-Learn\n",
        "# X: The features (Inputs)\n",
        "# y: The target (AQI)\n",
        "# test_size=0.2: Reserve 20% of the data for testing (Evaluation)\n",
        "# random_state=42: \"Lock\" the randomization so we get the same split every time we run this code\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# VISUAL CONFIRMATION DASHBOARD (Donut Chart)\n",
        "\n",
        "# Initialize the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add a Pie Chart trace (configured as a Donut Chart)\n",
        "fig.add_trace(go.Pie(\n",
        "    # Define labels for the two slices\n",
        "    labels=[\"Training Set\", \"Testing Set\"],\n",
        "    # Define the values (counts) for each slice\n",
        "    values=[len(X_train), len(X_test)],\n",
        "\n",
        "    # Donut configuration\n",
        "    hole=0.6, # Size of the center hole (0.6 = 60% radius)\n",
        "\n",
        "    # Styling\n",
        "    marker=dict(\n",
        "        colors=[\"#3b82f6\", \"#ef4444\"], # Blue for Train, Red for Test\n",
        "        line=dict(color=\"white\", width=3) # White border between slices\n",
        "    ),\n",
        "\n",
        "    # Text labels\n",
        "    textinfo=\"label+percent\", # Show \"Training Set\" and \"80%\"\n",
        "    textfont=dict(size=18, color=\"white\"), # White font\n",
        "\n",
        "    # Custom Tooltip\n",
        "    hovertemplate=\"<b>%{label}</b><br>Samples: %{value:,}<br>Percentage: %{percent}<extra></extra>\"\n",
        "))\n",
        "\n",
        "# Configure the overall layout\n",
        "fig.update_layout(\n",
        "    # HTML-styled title with subtitle\n",
        "    title=\"<b>2. Train-Test Split Complete</b><br><sub>80% Training ‚Ä¢ 20% Testing ‚Ä¢ Random State = 42</sub>\",\n",
        "    title_x=0.5, # Center the title\n",
        "\n",
        "    # Dark Theme settings\n",
        "    paper_bgcolor=\"#0f172a\", # Dark blue background\n",
        "    font=dict(color=\"white\", family=\"Arial\"), # White text\n",
        "\n",
        "    # Dimensions\n",
        "    height=400,\n",
        "    margin=dict(t=120, b=40) # Add top margin for title\n",
        ")\n",
        "\n",
        "# Add a Summary Annotation at the bottom\n",
        "fig.add_annotation(\n",
        "    text=f\"Training: <b>{len(X_train):,}</b> rows<br>Testing: <b>{len(X_test):,}</b> rows\",\n",
        "    xref=\"paper\", yref=\"paper\", # Relative coordinates\n",
        "    x=0.5, y=-0.1, # Position: Bottom Center\n",
        "    showarrow=False, # No arrow\n",
        "    font=dict(size=15), # Font size\n",
        "    bgcolor=\"#1e40af\" # Dark blue background box\n",
        ")\n",
        "\n",
        "# Display the interactive chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "4qjHVjskPZz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "Now, I have four key objects: `X_train`, `X_test`, `y_train`, and `y_test`. The raw `X` and `y` stay the same, but my modeling pipeline changes from a single dataset to a proper train-test structure. The Plotly donut chart shows the 80/20 split and the exact row counts, providing a clear reference for all future training, tuning, and performance reporting."
      ],
      "metadata": {
        "id": "cR2jQ4qbgtSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.3 TRAIN BASELINE MODEL (LINEAR REGRESSION)**\n",
        "I need to clean and align the target variable before training. A regression model cannot learn useful patterns if the ‚Äúanswer‚Äù (AQI) is missing or doesn't match its input features. Dropping rows with null `aqi` ensures that every training example has a valid target. Otherwise, the model would be optimizing against undefined labels, which is mathematically wrong and would distort performance metrics. Redefining X from numeric columns and removing `aqi` and `aqi_bucket_encoded` helps prevent leakage and retains only real predictors. Filling in remaining feature gaps with the mean is a necessary step so the model sees a complete numeric matrix. Most algorithms, including linear regression, cannot handle NaNs. Re-splitting the data after this cleaning ensures that X and y are perfectly aligned and free of missing values. Training a simple baseline Linear Regression at this stage is crucial. It provides a reference level of performance (R¬≤, RMSE) for comparing more complex models (Random Forest, Gradient Boosting, XGBoost). Without this process, any downstream model evaluation would be based on a possibly dirty or misaligned target, which would compromise the validity of the entire modeling phase."
      ],
      "metadata": {
        "id": "2E62aO9JPmnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üõ°Ô∏è fixing Target Variable (y)...\")\n",
        "\n",
        "# DROP ROWS WHERE TARGET IS MISSING\n",
        "# We cannot train on data where we don't know the answer.\n",
        "# We go back to 'df' (which now contains the encoded column) to ensure X and y stay aligned when we drop rows.\n",
        "df_clean_target = df.dropna(subset=['aqi'])\n",
        "\n",
        "print(f\"   Original Rows: {len(df)}\")\n",
        "print(f\"   Cleaned Rows:  {len(df_clean_target)} (Dropped {len(df) - len(df_clean_target)} rows with missing AQI)\")\n",
        "\n",
        "# REDEFINE X AND y\n",
        "# Select only numeric columns for features\n",
        "X_numeric = df_clean_target.select_dtypes(include=[np.number])\n",
        "\n",
        "# Define Target (y)\n",
        "y = df_clean_target['aqi']\n",
        "\n",
        "# Define Features (X) - Drop target and potential leakage columns\n",
        "cols_to_drop = ['aqi', 'aqi_bucket_encoded']\n",
        "X = X_numeric.drop(columns=[c for c in cols_to_drop if c in X_numeric.columns])\n",
        "\n",
        "# IMPUTE FEATURES (Safety Net)\n",
        "# Even though we dropped rows with missing y, X might still have gaps.\n",
        "# We fill those with the average.\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# RE-SPLIT DATA\n",
        "# Split the now perfectly aligned and clean data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"‚úÖ Data Ready. Training on {X_train.shape[0]} rows...\")\n",
        "\n",
        "# TRAIN BASELINE MODEL\n",
        "baseline = LinearRegression()\n",
        "baseline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# VISUAL CONFIRMATION DASHBOARD\n",
        "\n",
        "fig = go.Figure(go.Indicator(\n",
        "    mode=\"number+gauge\",\n",
        "    value=100,\n",
        "    number={'suffix': \"%\", 'font': {'size': 80}},\n",
        "    gauge={'axis': {'range': [0, 100]}, 'bar': {'color': \"#10b981\"}},\n",
        "    title={'text': \"<b>Baseline Model Trained</b><br><span style='font-size:0.9em;color:#cbd5e1'>Linear Regression ‚Ä¢ Target Cleaned</span>\"}\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    height=300,\n",
        "    paper_bgcolor=\"#0f172a\",\n",
        "    font={'color': \"white\"},\n",
        "    margin=dict(t=100, b=40, l=40, r=40)\n",
        ")\n",
        "\n",
        "fig.add_annotation(\n",
        "    text=\"Success: Dropped rows with missing AQI\",\n",
        "    xref=\"paper\", yref=\"paper\", x=0.5, y=-0.15,\n",
        "    showarrow=False, font=dict(size=14), bgcolor=\"#065f46\"\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# PRINT SCORES\n",
        "y_pred = baseline.predict(X_test)\n",
        "print(\"\\nüìä BASELINE MODEL SCORES:\")\n",
        "print(f\"R¬≤ Score: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"RMSE:     {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")"
      ],
      "metadata": {
        "id": "ntEf3xXAPnEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, all rows with missing `aqi` have been removed, resulting in `df_clean_target`, a dataset with a clean target. X has been rebuilt from purely numeric, non-leaky features, filled in to remove any remaining NaNs, and split into `X_train`, `X_test`, `y_train`, and `y_test`. A baseline Linear Regression model has been fitted, and its R¬≤ and RMSE on the test set are now available as scores. The indicator dashboard shows that target cleaning and baseline training are complete. The modeling pipeline is now based on a fully valid target and feature set."
      ],
      "metadata": {
        "id": "eBGJHfi9hKw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.4 EVALUATE PERFORMANCE**\n",
        "I need to evaluate the baseline model on unseen data to determine if it truly explains AQI variation or just memorizes the training set. Generating predictions on `X_test` and calculating MAE, RMSE, and R¬≤ provides a clear answer to \"How good is this model, really?\" MAE and RMSE indicate the typical size of the error in AQI units, which stakeholders can easily understand (‚Äúon average, predictions are off by about X AQI points‚Äù). R¬≤ shows what portion of AQI variance my features explain. Without it, I cannot fairly compare models or argue that my chosen predictors capture most of the signal. Using a threshold (like R¬≤ ‚âà 0.7) and visualizing these metrics in a scorecard helps me assess whether the baseline is strong enough or needs improvement. It also sets a standard that more complex models (Random Forest, XGBoost, etc.) must surpass to justify their added complexity."
      ],
      "metadata": {
        "id": "6lKrEbpMPvqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print status message\n",
        "print(\"Evaluating Baseline Model Performance\")\n",
        "\n",
        "# GENERATE PREDICTIONS\n",
        "# Use the trained model ('baseline') to predict AQI values for the Test set (X_test).\n",
        "# These are the model's \"guesses\" on data it has never seen before.\n",
        "y_pred = baseline.predict(X_test)\n",
        "\n",
        "# CALCULATE METRICS\n",
        "# MAE (Mean Absolute Error): The average absolute difference between predicted and actual values.\n",
        "# If MAE = 20, the model is off by 20 AQI points on average.\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# RMSE (Root Mean Squared Error): Similar to MAE but penalizes large errors more heavily.\n",
        "# It is the standard deviation of the prediction errors.\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# R¬≤ (R-Squared): Represents the proportion of variance in the dependent variable (AQI)\n",
        "# predictable from the independent variables. 1.0 is perfect; 0.0 is useless.\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# VISUALIZATION DASHBOARD (Performance Scorecard)\n",
        "\n",
        "# Initialize the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add an Indicator Trace (Big Number Display)\n",
        "fig.add_trace(go.Indicator(\n",
        "    mode=\"number+delta\",       # Show the main number (R¬≤) and the difference (Delta)\n",
        "    value=r2,                  # The actual R¬≤ score calculated above\n",
        "\n",
        "    # Delta Configuration: Compare our score against a threshold (0.7)\n",
        "    # If R¬≤ > 0.7, the arrow will be green (success). If < 0.7, red (needs improvement).\n",
        "    delta={'reference': 0.7, 'position': \"top\"},\n",
        "\n",
        "    # Number Formatting: Add \"R¬≤ =\" prefix and set font size\n",
        "    number={'prefix': \"R¬≤ = \", 'font': {'size': 80}},\n",
        "\n",
        "    # Gauge Configuration (Optional visual bar, here mainly used for color setting)\n",
        "    gauge={'axis': {'range': [0, 1]}, 'bar': {'color': \"#f59e0b\"}}, # Amber color\n",
        "\n",
        "    # Title with Subtitle (HTML styling)\n",
        "    title={'text': \"<b>4. Model Performance</b><br><sub>Linear Regression Baseline</sub>\"}\n",
        "))\n",
        "\n",
        "# Add a text annotation box at the bottom to list all metrics clearly\n",
        "fig.add_annotation(\n",
        "    # Format the text with bold values and 2 decimal places\n",
        "    text=f\"MAE: <b>{mae:.2f}</b><br>RMSE: <b>{rmse:.2f}</b><br>R¬≤: <b>{r2:.4f}</b>\",\n",
        "    xref=\"paper\", yref=\"paper\", # Relative coordinates\n",
        "    x=0.5, y=-0.5,             # Position: Bottom Center\n",
        "    showarrow=False,            # No arrow needed\n",
        "    font=dict(size=16),         # Font size\n",
        "    bgcolor=\"#ea580c\",          # Orange background box\n",
        "    borderpad=12                # Padding inside the box\n",
        ")\n",
        "\n",
        "# Final Layout Adjustments for Dark Theme\n",
        "fig.update_layout(\n",
        "    height=350,              # Figure height\n",
        "    paper_bgcolor=\"#0f172a\", # Dark blue background\n",
        "    font_color=\"white\"       # White text\n",
        ")\n",
        "\n",
        "# Display the evaluation dashboard\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OixZzxujPwIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I have specific performance metrics (MAE, RMSE, R¬≤) for the baseline Linear Regression on the test set, along with an interactive dashboard summarizing them. The model and data remain the same, but I now clearly understand how well the baseline performs. I also have a visual, report-ready artifact to compare against more complex models later in the notebook."
      ],
      "metadata": {
        "id": "YPNqSr66iicb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.5 VISUALIZE PREDICTIONS (ACTUAL VS PREDICTED)**\n",
        "I need to visualize the actual versus predicted AQI to understand how the model makes errors, not just the average size of those errors. Numeric metrics like R¬≤ and RMSE summarize performance, but they do not show if the model consistently under-predicts high AQI days, struggles in specific ranges, or exhibits non-linear error patterns. Plotting predicted values against actual values and adding the 45¬∞ ‚Äúperfect prediction‚Äù line allows me to check calibration. If points cluster closely around the line, the model is accurate. If they spread out or curve away, I can identify where it is biased or weak. Coloring points by actual AQI further highlights whether extreme pollution events are more difficult to predict. Without this plot, I would miss visual evidence indicating whether the baseline is just noisy or fundamentally misrepresenting certain AQI levels. This insight is crucial for deciding if I should use more complex models and how to interpret their outputs."
      ],
      "metadata": {
        "id": "aTBvKWxYP5Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print status message\n",
        "print(\"Visualizing Predictions: Actual vs Predicted AQI\")\n",
        "\n",
        "# Initialize the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "\n",
        "# TRACE 1: THE PREDICTIONS (SCATTER DOTS)\n",
        "# We plot Actual values on X-axis and Predicted values on Y-axis.\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test,       # X: The real AQI values (Ground Truth)\n",
        "    y=y_pred,       # Y: The values our model guessed\n",
        "    mode='markers', # Use dots\n",
        "\n",
        "    # Marker Styling\n",
        "    marker=dict(\n",
        "        color=y_test,          # Color dots by the actual AQI (Darker = Higher Pollution)\n",
        "        colorscale='Viridis',  # Professional color scale (Blue -> Green -> Yellow)\n",
        "        size=8,                # Dot size\n",
        "        opacity=0.7,           # Semi-transparent to handle overlapping points\n",
        "        line=dict(width=1, color='white') # White border for clarity\n",
        "    ),\n",
        "\n",
        "    name=\"Predictions\", # Legend Label\n",
        "    # Tooltip: Shows exact values on hover\n",
        "    hovertemplate=\"Actual: %{x:.0f}<br>Predicted: %{y:.1f}<extra></extra>\"\n",
        "))\n",
        "\n",
        "\n",
        "# TRACE 2: THE PERFECT PREDICTION LINE (DIAGONAL)\n",
        "\n",
        "# We draw a line from (Min, Min) to (Max, Max).\n",
        "# Any point falling exactly on this line represents a PERFECT prediction (Actual = Predicted).\n",
        "min_val, max_val = y_test.min(), y_test.max() # Find the range of the data\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val, max_val], # X-coordinates: Start and End\n",
        "    y=[min_val, max_val], # Y-coordinates: Start and End\n",
        "    mode=\"lines\",         # Draw a line\n",
        "    # Line Styling: Red, Thick, Dashed\n",
        "    line=dict(color=\"#dc2626\", width=4, dash=\"dash\"),\n",
        "    name=\"Perfect Prediction\"\n",
        "))\n",
        "\n",
        "# LAYOUT CONFIGURATION\n",
        "\n",
        "fig.update_layout(\n",
        "    # Title with Subtitle (HTML styling)\n",
        "    title=\"<b>Actual vs Predicted AQI</b><br><sub>Points near red line = Accurate predictions</sub>\",\n",
        "    title_x=0.5,           # Center the title\n",
        "\n",
        "    # Axis Labels\n",
        "    xaxis_title=\"Actual AQI\",\n",
        "    yaxis_title=\"Predicted AQI\",\n",
        "\n",
        "    # Dimensions and Background\n",
        "    height=600,            # Figure height\n",
        "    paper_bgcolor=\"#f8fafc\", # Light grey outer background\n",
        "    plot_bgcolor=\"white\",    # White inner plot background\n",
        "    hovermode=\"closest\",     # Tooltip highlights nearest point\n",
        "    font=dict(family=\"Arial\", size=14), # Global font\n",
        "    margin=dict(t=120, b=80, l=80, r=80) # Margins\n",
        ")\n",
        "\n",
        "\n",
        "# ANNOTATION (PERFORMANCE SCORE)\n",
        "\n",
        "# Add a box displaying the R¬≤ score directly on the chart\n",
        "fig.add_annotation(\n",
        "    text=f\"R¬≤ = {r2:.4f}<br>Closer to 1.0 = Better fit\", # Display the score\n",
        "    xref=\"paper\", yref=\"paper\", # Relative coordinates\n",
        "    x=0.05, y=0.95,             # Position: Top-Left\n",
        "    showarrow=False,            # No arrow\n",
        "    font=dict(size=15, color=\"#16a34a\"), # Green text\n",
        "    bgcolor=\"#ecfdf5\",          # Light green background box\n",
        "    bordercolor=\"#16a34a\"       # Green border\n",
        ")\n",
        "\n",
        "# Display the final chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2SzkW5KnP5ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, the data and model remain the same, but now I have a scatter plot that compares actual versus predicted AQI with a reference line and an annotated R¬≤ score. This provides an immediate visual check of prediction quality and bias across the AQI range, enhancing my ability to evaluate the baseline model and support further improvements."
      ],
      "metadata": {
        "id": "k8kDm1IejBCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.6 TRAIN ADVANCED MODEL - RANDOM FOREST**"
      ],
      "metadata": {
        "id": "jeytSHetUb27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.6.1 TRAIN RANDOM FOREST (Advanced Model)**\n",
        "I need to train a Random Forest regressor to find out if a more powerful, non-linear model can capture complex relationships between pollutants and AQI that linear regression cannot. AQI is influenced by different interactions and thresholds, such as combinations of PM2.5, PM10, gases, and seasonal features. These factors are often poorly represented by a strictly linear model. Random Forests combine multiple decision trees, automatically modeling non-linear effects and feature interactions while remaining relatively resistant to noise and outliers. Setting `n_estimators=100` and `n_jobs=-1` creates a decent ensemble within a reasonable runtime. Without testing a tree-based model like this, I wouldn't know if my baseline is underusing the information from the engineered features or if a more complex model provides significantly better predictive performance."
      ],
      "metadata": {
        "id": "1f9nFhK4U69i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Random Forest Regressor (100 trees)...\")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "print(\"Random Forest trained successfully!\")"
      ],
      "metadata": {
        "id": "VszewZKeUcd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I have a trained `rf` model fitted on `X_train` and `y_train`, along with new predictions `y_pred_rf` for `X_test`. The data itself hasn't changed, but my modeling tools have grown: I now have a non-linear benchmark to compare against the linear model. This allows for a fair assessment of whether ensemble methods truly enhance AQI prediction accuracy."
      ],
      "metadata": {
        "id": "8ZAE5MzVjTng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.6.2 PERFORMANCE COMPARISON**\n",
        "I need to compare Random Forest directly to the linear baseline. This will show if the added complexity of the model truly leads to better predictions. By calculating MAE, RMSE, and especially R¬≤ for the Random Forest on the same test set, and then plotting both models‚Äô R¬≤ next to each other, I can answer an important question: is the ensemble significantly better, or just slightly different? The bar chart makes this comparison easy to see. Highlighting the R¬≤ difference in the subtitle along with a ‚Äú*winner*‚Äù badge creates a clear conclusion instead of a vague statement. Without this direct comparison, I wouldn‚Äôt be able to justify favoring Random Forest over Linear Regression in my report, nor could I claim that tree-based, non-linear models are better suited for AQI prediction with this dataset."
      ],
      "metadata": {
        "id": "RafuuIQ8VIVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We evaluate the Random Forest model using the same Test Set (X_test, y_test)\n",
        "# to ensure a fair \"apples-to-apples\" comparison with Linear Regression.\n",
        "\n",
        "# MAE (Mean Absolute Error): Average magnitude of errors (lower is better)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# RMSE (Root Mean Squared Error): Penalizes large errors more than MAE (lower is better)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "\n",
        "# R¬≤ (Coefficient of Determination): How well the model explains variance (higher is better)\n",
        "# 1.0 = Perfect fit, 0.0 = Useless model\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Retrieve the R¬≤ score of the Baseline Model (Linear Regression) we calculated earlier\n",
        "# We assume the variable 'r2' holds the score from the previous step\n",
        "r2_baseline = r2\n",
        "\n",
        "# INITIALIZE COMPARISON CHART\n",
        "fig = go.Figure()\n",
        "\n",
        "# ADD BAR CHART TRACE\n",
        "fig.add_trace(go.Bar(\n",
        "    # X-Axis: Categories comparing the two models\n",
        "    x=['Linear Regression', 'Random Forest'],\n",
        "\n",
        "    # Y-Axis: The R¬≤ scores (height of the bars)\n",
        "    y=[r2_baseline, r2_rf],\n",
        "\n",
        "    # Text Labels: Display the exact score on top of each bar (formatted to 4 decimals)\n",
        "    text=[f\"{r2_baseline:.4f}\", f\"{r2_rf:.4f}\"],\n",
        "\n",
        "    # Position the text *outside* the bar (on top) for readability\n",
        "    textposition='outside',\n",
        "\n",
        "    # Styling the Bars\n",
        "    marker=dict(\n",
        "        # Color Logic: Grey (#94a3b8) for the \"Old/Baseline\" model to make it recede\n",
        "        #              Bright Green (#10b981) for the \"New/Champion\" model to highlight success\n",
        "        color=['#94a3b8', '#10b981'],\n",
        "        # Add a white border to make bars pop against the dark background\n",
        "        line=dict(width=3, color='white')\n",
        "    ),\n",
        "\n",
        "    # Custom Tooltip: Shows details when hovering\n",
        "    # <extra></extra> hides the default \"trace 0\" label\n",
        "    hovertemplate=\"<b>%{x}</b><br>R¬≤ Score: <b>%{y:.4f}</b><br><extra></extra>\"\n",
        "))\n",
        "\n",
        "# LAYOUT (DARK THEME)\n",
        "fig.update_layout(\n",
        "    # Dynamic Title: Automatically calculates and displays the numeric improvement\n",
        "    title={\n",
        "        'text': \"<b>Model Accuracy Comparison</b><br>\"\n",
        "                # HTML styling is used to color the improvement number green\n",
        "                f\"<sub>Random Forest wins by <span style='color:#10b981'>+{(r2_rf - r2_baseline):.4f}</span> R¬≤ points</sub>\",\n",
        "        'x': 0.5, # Center the title\n",
        "        'font': dict(size=22) # Large, readable font\n",
        "    },\n",
        "\n",
        "    # Y-Axis Configuration\n",
        "    # We fix the range from 0 to 1 because R¬≤ scores are typically within this range\n",
        "    yaxis=dict(title=\"R¬≤ Score\", range=[0, 1.1], gridcolor=\"#334155\"), # 1.1 gives headroom for text\n",
        "    xaxis=dict(title=\"\"), # Hide X-axis title (Labels are self-explanatory)\n",
        "\n",
        "    # Styling Dimensions & Colors\n",
        "    height=600,              # Comfortable viewing height\n",
        "    paper_bgcolor=\"#0f172a\", # Dark blue outer background\n",
        "    plot_bgcolor=\"#1e293b\",  # Slightly lighter plot area\n",
        "    font=dict(color=\"#e2e8f0\", family=\"Arial\"), # White/Grey text for contrast\n",
        "\n",
        "    # Margins: Adjusted to ensure the title and \"Winner\" badge fit comfortably\n",
        "    margin=dict(t=140, b=80, l=100, r=100)\n",
        ")\n",
        "\n",
        "# BADGE ANNOTATION\n",
        "# This draws a visual indicator pointing to the best model\n",
        "fig.add_annotation(\n",
        "    text=\"WINNER\", # The badge text\n",
        "    x=\"Random Forest\", # Position it over the Random Forest bar\n",
        "    y=r2_rf + 0.1,    # Position it slightly above the bar height (adjusted offset)\n",
        "\n",
        "    # Arrow Styling\n",
        "    showarrow=True,\n",
        "    arrowhead=7,       # Style 7 is a clean pointer\n",
        "    arrowcolor=\"#10b981\", # Match the green theme\n",
        "    arrowsize=3,\n",
        "\n",
        "    # Text Box Styling\n",
        "    font=dict(size=20, color=\"#10b981\"), # Bold green text\n",
        "    bgcolor=\"#166534\", # Dark green background for the badge\n",
        "    borderpad=10       # Padding inside the badge\n",
        ")\n",
        "\n",
        "# DISPLAY THE CHART\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "GPAsRt9IVI02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I now have performance metrics for the Random Forest (`mae_rf`, `rmse_rf`, `r2_rf`) and a visual bar chart comparing its R¬≤ to the baseline Linear Regression. The underlying data and models have not changed, but I now have clear proof that Random Forest either performs better or does not perform better than the baseline. This is shown through the R¬≤ gap and visually marked by the ‚Äúwinner‚Äù label, which strengthens my argument for choosing a model."
      ],
      "metadata": {
        "id": "NhTyiuUNlKKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.6.3 FEATURE IMPORTANCE ‚Äì What Really Drives AQI?**\n",
        "I need to look at feature importance from the Random Forest to shift from ‚Äúthe model works‚Äù to ‚Äúthese are the variables actually driving AQI in this dataset.‚Äù Random Forests provide importance scores that show how much each feature helps reduce prediction error. If I ignore them, I might have a strong model but a poor explanation. By ranking and plotting the top 10 features, I get clear visual evidence of which pollutants and engineered variables are most important. Typically, PM2.5 and its related components stand out as the main drivers. This information is crucial for my report and stakeholders: it backs up claims like ‚ÄúPM2.5 is the primary driver of hazardous AQI‚Äù with evidence from the model rather than just relying on intuition, and it connects machine-learning results to real-world pollution sources and possible solutions."
      ],
      "metadata": {
        "id": "GA8ZpoadVwGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT & ORGANIZE FEATURE IMPORTANCE\n",
        "# The Random Forest model ('rf') calculates how useful each feature was for prediction.\n",
        "# We extract these scores using the .feature_importances_ attribute.\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to link Feature Names (columns) with their Importance Scores.\n",
        "feat_imp = pd.DataFrame({\n",
        "    'Feature': X.columns,     # The names of the pollutant columns (PM2.5, NO2, etc.)\n",
        "    'Importance': importances # The score (Higher = More influential)\n",
        "}).sort_values('Importance', ascending=True).tail(10)\n",
        "# .sort_values(...): We sort them so the bar chart looks organized (smallest to largest).\n",
        "# .tail(10): We only keep the \"Top 10\" most important features to avoid clutter.\n",
        "\n",
        "# INITIALIZE THE HORIZONTAL BAR CHART\n",
        "fig = go.Figure(go.Bar(\n",
        "    y=feat_imp['Feature'],    # Y-axis: Feature Names (Horizontal orientation uses Y for categories)\n",
        "    x=feat_imp['Importance'], # X-axis: The length of the bar represents Importance\n",
        "    orientation='h',          # 'h' = Horizontal Bar Chart (easier to read long labels)\n",
        "\n",
        "    # Text Labels\n",
        "    text=feat_imp['Importance'].round(4), # Show the exact score (rounded to 4 decimals)\n",
        "    textposition='outside',   # Place the text at the end of the bar for clarity\n",
        "\n",
        "    # Bar Styling\n",
        "    marker=dict(\n",
        "        color=feat_imp['Importance'], # Color bars dynamically based on their score\n",
        "        colorscale='Viridis',         # Use the 'Viridis' palette (Yellow = High, Purple = Low)\n",
        "        line=dict(color='white', width=1) # Add a thin white border for a clean look\n",
        "    ),\n",
        "\n",
        "    # Custom Tooltip (Hover Info)\n",
        "    # Shows the Feature Name and precise score when hovering\n",
        "    hovertemplate=\"<b>%{y}</b><br>Importance: <b>%{x:.4f}</b><extra></extra>\"\n",
        "))\n",
        "\n",
        "# LAYOUT CONFIGURATION (DARK THEME)\n",
        "fig.update_layout(\n",
        "    # Chart Title with Subtitle\n",
        "    title=\"<b>Top 10 Drivers of AQI</b><br><sub>Random Forest reveals true pollution sources</sub>\",\n",
        "    title_x=0.5, # Center the title\n",
        "\n",
        "    # Axis Labels\n",
        "    xaxis_title=\"Feature Importance Score\",\n",
        "\n",
        "    # Appearance Settings\n",
        "    height=650,              # Taller height to fit all labels comfortably\n",
        "    paper_bgcolor=\"#0f172a\", # Dark blue outer background\n",
        "    plot_bgcolor=\"#1e293b\",  # Slightly lighter plot area\n",
        "    font=dict(color=\"#e2e8f0\"), # Light grey text for readability on dark background\n",
        "\n",
        "    # Margins: Extra left margin (l=180) to ensure long feature names aren't cut off\n",
        "    margin=dict(l=180, r=100, t=120, b=60)\n",
        ")\n",
        "\n",
        "# HIGHLIGHT THE #1 PREDICTOR\n",
        "# We identify the single most important row (the last one, since we sorted ascending)\n",
        "top_feat = feat_imp.iloc[-1]\n",
        "\n",
        "# Add an annotation arrow pointing to the top bar\n",
        "fig.add_annotation(\n",
        "    text=f\"PM2.5 dominates<br>{top_feat['Feature']} = {top_feat['Importance']:.3f}\", # Dynamic text\n",
        "    x=top_feat['Importance'] + 0.05, # Position text slightly to the right of the bar\n",
        "    y=top_feat['Feature'],           # Align vertically with the top feature\n",
        "\n",
        "    # Arrow Styling\n",
        "    showarrow=True,\n",
        "    arrowhead=2,\n",
        "    arrowcolor=\"#f59e0b\", # Orange arrow to stand out against blue background\n",
        "\n",
        "    # Text Box Styling\n",
        "    font=dict(size=14, color=\"#f59e0b\"), # Orange text\n",
        "    bgcolor=\"rgba(251,146,60,0.2)\"       # Semi-transparent orange background\n",
        ")\n",
        "\n",
        "# DISPLAY CHART\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "R-gyo3jbVwlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, I have a `feat_imp` table that lists the top 10 most important features and an interactive horizontal bar chart displaying their importance scores. There‚Äôs also an annotation that highlights the single most influential variable. The Random Forest and data remain the same, but I now understand which inputs the model depends on most. This lets me interpret predictions, talk about pollution drivers, and prioritize variables in both my narrative and any future policy recommendations."
      ],
      "metadata": {
        "id": "0ldtnf43l9BA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3.6.4 ACTUAL vs PREDICTED ‚Äì Final Model Quality Check**\n",
        "I need to create this final Actual vs Predicted plot for the Random Forest. This will help me visually confirm that my chosen ‚Äúbest‚Äù model is not only strong numerically but also behaves appropriately across the entire AQI range. When points are closely grouped around the diagonal line, it indicates that the model is well-calibrated: high AQI days are predicted as high, low days as low, with no clear bias. Coloring by actual AQI provides an extra check to ensure that extreme events are not consistently under- or over-predicted. Including R¬≤ and MAE in the annotation makes the figure a clear ‚Äúfinal evidence‚Äù slide that connects accuracy and error magnitude. Without this, my assertion that Random Forest is the best model would rely solely on metrics, not on a clear visual of prediction quality."
      ],
      "metadata": {
        "id": "Y_2Vaa1AWOnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE THE FIGURE\n",
        "fig = go.Figure()\n",
        "\n",
        "# TRACE 1: THE PREDICTIONS (SCATTER DOTS)\n",
        "# This creates the main cloud of points.\n",
        "# X-axis = The Truth (Actual AQI)\n",
        "# Y-axis = The Model's Guess (Predicted AQI)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test,       # Actual values from the test set\n",
        "    y=y_pred_rf,    # Predicted values from the Random Forest model\n",
        "    mode='markers', # Use dots (markers) instead of lines\n",
        "\n",
        "    # Marker Styling:\n",
        "    # color=y_test: Colors the dots based on the Actual AQI (Darker = Higher Pollution)\n",
        "    # colorscale='Plasma': A vibrant purple-to-yellow scale that looks professional\n",
        "    # opacity=0.7: Makes dots semi-transparent so you can see where they cluster/overlap\n",
        "    marker=dict(\n",
        "        color=y_test,\n",
        "        colorscale='Plasma',\n",
        "        size=8,\n",
        "        opacity=0.7,\n",
        "        line=dict(width=1, color='white') # Thin white border makes each dot pop\n",
        "    ),\n",
        "\n",
        "    name=\"Predictions\", # Legend label\n",
        "    # Custom Tooltip: Shows exact values when you hover over a dot\n",
        "    hovertemplate=\"Actual: %{x:.0f}<br>Predicted: %{y:.1f}<extra></extra>\"\n",
        "))\n",
        "\n",
        "# TRACE 2: THE \"PERFECT PREDICTION\" LINE\n",
        "# We draw a diagonal dashed line. If a point falls exactly on this line,\n",
        "# it means the Model's Prediction was IDENTICAL to the Actual Value.\n",
        "min_val = min(y_test.min(), y_pred_rf.min()) # Find the lowest value in the data\n",
        "max_val = max(y_test.max(), y_pred_rf.max()) # Find the highest value in the data\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val, max_val], # Start point (min) to End point (max)\n",
        "    y=[min_val, max_val], # Same Y values creates a 45-degree diagonal line\n",
        "    mode='lines',         # Draw a line, not dots\n",
        "    line=dict(color=\"#10b981\", width=5, dash='dash'), # Green, thick, dashed line\n",
        "    name=\"Perfect Prediction\"\n",
        "))\n",
        "\n",
        "# LAYOUT CONFIGURATION\n",
        "fig.update_layout(\n",
        "    # Dynamic Title: Automatically inserts the R¬≤ score into the subtitle\n",
        "    title=\"<b>Final Model: Actual vs Predicted AQI</b><br><sub>Random Forest ‚Ä¢ R¬≤ = {r2_rf:.4f}</sub>\".format(r2_rf=r2_rf),\n",
        "    title_x=0.5, # Center the title\n",
        "\n",
        "    # Axis Labels\n",
        "    xaxis_title=\"Actual AQI\",\n",
        "    yaxis_title=\"Predicted AQI\",\n",
        "\n",
        "    # Dimensions and Background Colors\n",
        "    height=600,              # Set figure height\n",
        "    paper_bgcolor=\"#f8fafc\", # Light grey outer background\n",
        "    plot_bgcolor=\"white\",    # White inner plotting area\n",
        "    hovermode=\"closest\",     # Tooltip appears for the point nearest to cursor\n",
        "    font=dict(family=\"Arial\", size=14) # Global font settings\n",
        ")\n",
        "\n",
        "# STATISTICAL ANNOTATION (BADGE)\n",
        "# Adds a box in the top-left corner displaying the key metrics (R¬≤ and MAE)\n",
        "fig.add_annotation(\n",
        "    text=f\"Excellent Fit<br>R¬≤ = {r2_rf:.4f}<br>MAE = {mae_rf:.2f}\", # The text to display\n",
        "    xref=\"paper\", yref=\"paper\", # Use relative coordinates (0 to 1)\n",
        "    x=0.05, y=0.95,             # Position: Top-Left\n",
        "\n",
        "    # Styling the text box\n",
        "    font=dict(size=16, color=\"#10b981\"), # Green text to indicate success\n",
        "    bgcolor=\"#ecfdf5\",          # Light green background\n",
        "    bordercolor=\"#10b981\"       # Green border\n",
        ")\n",
        "\n",
        "# RENDER THE CHART\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "iOMAsbQNWPB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, the model and data stay the same, but I now have a polished, publication-ready chart showing the Random Forest‚Äôs actual vs predicted AQI, a reference line for perfect predictions, and an annotation summarizing R¬≤ and MAE. This serves as the key visual for presenting and defending the final model in both the notebook and the written report."
      ],
      "metadata": {
        "id": "Z4hhJU6QmM4L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPrua7K1Jzc_"
      },
      "source": [
        "# Streamlit Application Development\n",
        "I need to build this Streamlit app to meet the assessment requirement for a multipage GUI and to create an interactive tool from my analysis and model instead of a static notebook. The app allows non-technical users to explore the India air quality data, check seasonal and pollutant patterns, train a Random Forest model, and generate live AQI predictions using sliders, all within a browser. Loading and cleaning the processed CSV in the app, with strong column detection and median imputation, makes sure that what users see matches the preprocessed dataset used in the notebook. The tabbed layout (\"Home\", \"EDA\", \"Seasonal\", \"Model\", \"Predict\", \"Map\", \"About\") organizes the experience around the main learning outcomes: understanding the data, analyzing patterns, building and evaluating a model, and deploying it in a decision-support interface. The geospatial map and seasonal plots turn technical findings, such as hotspots and winter peaks, into visuals that stakeholders can quickly interpret. Without this app, my work would only reach the point of code and figures. I would not show the ability to combine data, ML, and visualization into a functional software product, which is essential for CMP7005.\n",
        "\n",
        "\n",
        "The final stage of the data lifecycle is **Deployment**. A machine learning model is useless if it sits in a notebook; it needs to be accessible to end-users (business stakeholders, policymakers, public) to drive decision-making.\n",
        "\n",
        "I am building a **Streamlit** web application to serve as the interface for my project. This app has three core functions:\n",
        "1.  **Data Explorer:** Allows users to view and filter the raw cleaned data.\n",
        "2.  **Visualisation Dashboard:** Displays the key charts generated during EDA (Trends, Distributions).\n",
        "3.  **Predictive Tool:** A \"What-If\" calculator where users can input pollutant levels (PM2.5, NO2, etc.) and get an instant Air Quality prediction from my trained machine learning model.\n",
        "\n",
        "This transforms my static analysis into a dynamic product.\n",
        "\n",
        "**References:**\n",
        "\n",
        "i) Streamlit (2023) *Streamlit Documentation*. Available at: https://docs.streamlit.io/ (Accessed: 4 December 2025)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7JK-RAZJyrH"
      },
      "outputs": [],
      "source": [
        "# INDIA AIR QUALITY ANALYSIS APP (CMP7005 WRT1)\n",
        "# DEVELOPED BY: MD RABIUL ALAM\n",
        "# STUDENT ID: ST20316895\n",
        "\n",
        "# IMPORTING LIBRARIES\n",
        "import streamlit as st                  # Import Streamlit: The main library to build the web app interface\n",
        "import pandas as pd                     # Import Pandas: Used for reading CSVs and manipulating dataframes\n",
        "import numpy as np                      # Import Numpy: Used for numerical operations like arrays and math functions\n",
        "import plotly.express as px             # Import Plotly Express: High-level library for creating quick, interactive charts\n",
        "import plotly.graph_objects as go       # Import Plotly Graph Objects: Lower-level library for creating detailed custom charts (like gauges)\n",
        "from sklearn.model_selection import train_test_split # Import tool to split data into Training and Testing sets\n",
        "from sklearn.ensemble import RandomForestRegressor   # Import the Machine Learning algorithm (Random Forest)\n",
        "from sklearn.metrics import r2_score    # Import the metric to evaluate how good the model is (R-squared score)\n",
        "import joblib                           # Import Joblib: Used to save the trained model to a file and load it back later\n",
        "import warnings                         # Import warnings library to manage system alerts\n",
        "\n",
        "# Suppress warnings (like deprecation warnings) to keep the app interface clean for the user\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# APP CONFIGURATION & STYLING\n",
        "\n",
        "# Configure the page settings:\n",
        "# page_title: Shows in the browser tab\n",
        "# layout=\"wide\": Uses the full width of the screen (better for dashboards)\n",
        "# page_icon: Sets the favicon (little icon in the browser tab)\n",
        "st.set_page_config(page_title=\"India Air Quality\", layout=\"wide\", page_icon=\"üå§Ô∏è\")\n",
        "\n",
        "# Inject Custom HTML & CSS to style the app and give it a professional look\n",
        "# st.markdown allows us to write raw HTML/CSS directly into the app\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* Style the main header container */\n",
        "    .header {\n",
        "        background: linear-gradient(90deg, #001f3f, #003366); /* Dark blue gradient background */\n",
        "        padding: 35px;                                        /* Add space inside the box */\n",
        "        border-radius: 18px;                                  /* Round the corners */\n",
        "        color: white;                                         /* White text color */\n",
        "        text-align: center;                                   /* Center align the text */\n",
        "        box-shadow: 0 12px 35px rgba(0,0,0,0.4);              /* Add a shadow for 3D effect */\n",
        "        margin-bottom: 40px;                                  /* Add space below the header */\n",
        "    }\n",
        "\n",
        "    /* Style the logo image inside the header */\n",
        "    .header img {height: 110px; margin-right: 25px;}\n",
        "\n",
        "    /* Style the main title text */\n",
        "    .title {font-size: 56px; font-weight: bold; margin: 0;}\n",
        "\n",
        "    /* Style the subtitle text (Module code, Student ID) */\n",
        "    .subtitle {font-size: 28px; margin: 12px 0; color: #e2e8f0;}\n",
        "\n",
        "    /* Style the Navigation Tabs */\n",
        "    .stTabs [data-testid=\"stTab\"] {\n",
        "        background: #001f3f;           /* Dark blue background for tabs */\n",
        "        color: white;                  /* White text */\n",
        "        border-radius: 12px 12px 0 0;  /* Round only the top corners */\n",
        "        padding: 16px 34px;            /* Add padding for size */\n",
        "        font-weight: bold;             /* Bold text */\n",
        "    }\n",
        "\n",
        "    /* Change color of the Active (Selected) Tab */\n",
        "    .stTabs [aria-selected=\"true\"] {background: #0074D9;} /* Lighter blue for active tab */\n",
        "\n",
        "    /* Style the Metric Cards (the boxes showing numbers) */\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #0074D9, #001f3f); /* Diagonal gradient */\n",
        "        padding: 35px;\n",
        "        border-radius: 22px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        box-shadow: 0 12px 30px rgba(0,0,0,0.3); /* Soft shadow */\n",
        "    }\n",
        "</style>\n",
        "\n",
        "<div class=\"header\">\n",
        "    <img src=\"https://www.cardiffmet.ac.uk/PublishingImages/logo.png\" alt=\"Cardiff Met\">\n",
        "    <div class=\"title\">India Air Quality Analysis</div>\n",
        "    <div class=\"subtitle\">CMP7005 ‚Ä¢ ST20316895 ‚Ä¢ 2025-26</div>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True) # unsafe_allow_html=True is required to render the HTML\n",
        "\n",
        "\n",
        "# DATA LOADING & CLEANING (ROBUST)\n",
        "\n",
        "# Define a function to load data and cache it\n",
        "# @st.cache_data ensures the data is loaded only once and stored in memory\n",
        "# This prevents reloading the CSV every time the user clicks a button, making the app faster\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    try:\n",
        "        # Load the CSV file into a Pandas DataFrame\n",
        "        df = pd.read_csv(\"India_Air_Quality_Final_Processed.csv\")\n",
        "\n",
        "        # Clean up column names by removing any leading/trailing spaces (common issue in CSVs)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "        # Robust Column Renaming\n",
        "        # The code below ensures the app works even if column names have different capitalization\n",
        "\n",
        "        # Search for a column containing 'AQI' but NOT 'BUCKET' (to avoid the categorical column)\n",
        "        aqi_col = next((c for c in df.columns if 'AQI' in c.upper() and 'BUCKET' not in c.upper()), None)\n",
        "        # If found, rename it to the standard 'AQI'\n",
        "        if aqi_col: df.rename(columns={aqi_col: 'AQI'}, inplace=True)\n",
        "\n",
        "        # Search for a column containing 'DATE' and rename it to 'Date'\n",
        "        date_col = next((c for c in df.columns if 'DATE' in c.upper()), None)\n",
        "        if date_col: df.rename(columns={date_col: 'Date'}, inplace=True)\n",
        "\n",
        "        # Search for a column containing 'CITY' and rename it to 'City'\n",
        "        city_col = next((c for c in df.columns if 'CITY' in c.upper()), None)\n",
        "        if city_col: df.rename(columns={city_col: 'City'}, inplace=True)\n",
        "\n",
        "        # Convert the 'Date' column to datetime objects so we can extract year/month later\n",
        "        # errors='coerce' turns unparseable dates into NaT (Not a Time) instead of crashing\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "        # Filtering Columns\n",
        "        # We only want to keep the Date, City, AQI, and numeric columns (pollutants)\n",
        "        # We select all numeric columns automatically\n",
        "        keep_cols = ['Date', 'City', 'AQI'] + df.select_dtypes(include='number').columns.tolist()\n",
        "        # Create a new dataframe with only these columns (using set to remove duplicates)\n",
        "        df = df[list(set(keep_cols))].copy()\n",
        "\n",
        "        # Data Cleaning\n",
        "        # CRITICAL: We cannot train a model if the Target (AQI) is missing.\n",
        "        # So we drop any rows where AQI is NaN.\n",
        "        df = df.dropna(subset=['AQI'])\n",
        "\n",
        "        # For feature columns (like PM2.5, NO2), we fill missing values with the Median.\n",
        "        # Using Median is safer than Mean because it's not affected by outliers.\n",
        "        numeric_cols = df.select_dtypes(include='number').columns.drop('AQI', errors='ignore')\n",
        "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "        # Return the cleaned dataframe\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        # If any error occurs during loading (e.g., file not found), show an error message\n",
        "        st.error(f\"Error loading data: {e}. Please ensure 'India_Air_Quality_Final_Processed.csv' is in the folder.\")\n",
        "        # Return an empty dataframe to prevent the app from crashing completely\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Call the function to load the data into the variable 'df'\n",
        "df = load_data()\n",
        "\n",
        "# Safety Check: If dataframe is empty (due to error), stop the app execution here\n",
        "if df.empty:\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "# MAIN APP TABS\n",
        "\n",
        "# Create the navigation tabs layout\n",
        "# This creates a list of tabs that the user can click to switch views\n",
        "tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([\n",
        "    \"Home\", \"EDA\", \"Seasonal\", \"Model\", \"Predict\", \"Map\", \"About\"\n",
        "])\n",
        "\n",
        "# TAB 1: PROJECT DASHBOARD\n",
        "with tab1:\n",
        "    st.header(\"Project Dashboard\") # Set the header for this tab\n",
        "\n",
        "    # Create 4 columns for the metric cards\n",
        "    c1, c2, c3, c4 = st.columns(4)\n",
        "\n",
        "    # Display Key Performance Indicators (KPIs) using our custom CSS style 'metric-card'\n",
        "    # .nunique() counts unique cities, .mean() calculates average, .max() finds the highest value\n",
        "    with c1: st.markdown(f'<div class=\"metric-card\">Records<br><h2>{len(df):,}</h2></div>', unsafe_allow_html=True)\n",
        "    with c2: st.markdown(f'<div class=\"metric-card\">Cities<br><h2>{df[\"City\"].nunique()}</h2></div>', unsafe_allow_html=True)\n",
        "    with c3: st.markdown(f'<div class=\"metric-card\">Avg AQI<br><h2>{df[\"AQI\"].mean():.1f}</h2></div>', unsafe_allow_html=True)\n",
        "    with c4: st.markdown(f'<div class=\"metric-card\">Peak AQI<br><h2>{df[\"AQI\"].max():.0f}</h2></div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Plot National Trends Line Chart\n",
        "    # We sample 5000 points (min) to ensure the chart remains fast and responsive\n",
        "    fig = px.line(df.sample(min(5000, len(df))), x='Date', y='AQI', color='City', title=\"National AQI Trends\")\n",
        "    # Display the chart using the full width of the container\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# TAB 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
        "with tab2:\n",
        "    st.header(\"Exploratory Data Analysis\")\n",
        "\n",
        "    # Select only numeric columns (excluding the target AQI) to see correlations between pollutants\n",
        "    numeric = df.select_dtypes(include='number').columns.drop('AQI', errors='ignore')\n",
        "\n",
        "    # Create a Correlation Heatmap\n",
        "    # df.corr() calculates the correlation matrix\n",
        "    fig = px.imshow(df[numeric].corr(), title=\"Pollutant Correlations\", color_continuous_scale=\"Blues\")\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# TAB 3: SEASONAL ANALYSIS\n",
        "with tab3:\n",
        "    st.header(\"Seasonal Patterns\")\n",
        "\n",
        "    # Feature Engineering: Extract Month from Date and map it to a Season name\n",
        "    # This helps analyze pollution based on Indian seasons (Winter, Monsoon, etc.)\n",
        "    df['Season'] = df['Date'].dt.month.map({\n",
        "        12:'Winter', 1:'Winter', 2:'Winter',\n",
        "        3:'Spring', 4:'Spring', 5:'Spring',\n",
        "        6:'Summer', 7:'Summer', 8:'Summer',\n",
        "        9:'Monsoon', 10:'Monsoon', 11:'Monsoon'\n",
        "    })\n",
        "\n",
        "    # Create a Box Plot to visualize the distribution of AQI across different seasons\n",
        "    # This shows the median, quartiles, and outliers for each season\n",
        "    fig = px.box(df, x='Season', y='AQI', color='Season', title=\"AQI by Season\")\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# TAB 4: MODEL TRAINING\n",
        "with tab4:\n",
        "    st.header(\"Model Training\")\n",
        "\n",
        "    # Define the Features (X) and Target (y)\n",
        "    # X: All numeric columns EXCEPT 'AQI' (the inputs)\n",
        "    X = df.select_dtypes(include='number').drop(columns=['AQI'], errors='ignore')\n",
        "    # y: The 'AQI' column (what we want to predict)\n",
        "    y = df['AQI']\n",
        "\n",
        "    # Create a button to trigger the training process\n",
        "    if st.button(\"Train Random Forest Model\", type=\"primary\"):\n",
        "        with st.spinner(\"Training...\"): # Show a loading spinner while training\n",
        "\n",
        "            # Initialize the Random Forest Regressor\n",
        "            # n_estimators=100: Create 100 decision trees\n",
        "            # n_jobs=-1: Use all available CPU cores for speed\n",
        "            model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "            # Train the model on the data\n",
        "            model.fit(X, y)\n",
        "\n",
        "            # Make predictions on the training data to calculate the score\n",
        "            pred = model.predict(X)\n",
        "            # Calculate R-squared score (1.0 is perfect, 0.0 is poor)\n",
        "            r2 = r2_score(y, pred)\n",
        "\n",
        "            # Save the trained model to a file so we can use it in the Predict tab\n",
        "            joblib.dump(model, \"aqi_model.pkl\")\n",
        "\n",
        "        # Show a success message with the R2 score\n",
        "        st.success(f\"Model Trained! R¬≤ = {r2:.4f}\")\n",
        "        # Show a celebration animation (balloons)\n",
        "        st.balloons()\n",
        "\n",
        "# TAB 5: LIVE PREDICTION\n",
        "with tab5:\n",
        "    st.header(\"Live AQI Prediction\")\n",
        "    try:\n",
        "        # Load the trained model from the file\n",
        "        model = joblib.load(\"aqi_model.pkl\")\n",
        "\n",
        "        # Get the feature names that the model expects (from the X dataframe)\n",
        "        X_features = df.select_dtypes(include='number').drop(columns=['AQI'], errors='ignore')\n",
        "        features = X_features.columns.tolist()\n",
        "\n",
        "        inputs = []\n",
        "        # Create sliders for the top 6 features to keep the UI clean\n",
        "        # Loops through the first 6 feature names\n",
        "        for f in features[:6]:\n",
        "            # Create a slider for each feature, default value 100\n",
        "            val = st.slider(f, 0, 500, 100)\n",
        "            inputs.append(val)\n",
        "\n",
        "        # Create a button to trigger the prediction\n",
        "        if st.button(\"Predict AQI\", type=\"primary\"):\n",
        "            # Prepare the input array for the model\n",
        "            # Since the model expects ALL features, we pad the remaining features with a default value of 50\n",
        "            full_inputs = inputs + [50]*(len(features)-len(inputs))\n",
        "            input_arr = np.array([full_inputs])\n",
        "\n",
        "            # Use the model to predict the AQI\n",
        "            pred = model.predict(input_arr)[0]\n",
        "\n",
        "            # Display the result in a large green header\n",
        "            st.markdown(f\"<h1 style='color:#10b981'>Predicted AQI: {pred:.1f}</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "            # Create a Gauge Chart to visualize the severity\n",
        "            fig = go.Figure(go.Indicator(\n",
        "                mode = \"gauge+number\",\n",
        "                value = pred,\n",
        "                title = {'text': \"AQI Level\"},\n",
        "                gauge = {'axis': {'range': [0, 500]}, 'bar': {'color': \"#10b981\"}}\n",
        "            ))\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        # If the model hasn't been trained yet (file not found), show an info message\n",
        "        st.info(f\"Please go to the 'Model' tab and train the model first. ({e})\")\n",
        "\n",
        "# TAB 6: GEOSPATIAL MAP\n",
        "with tab6:\n",
        "    st.header(\"Pollution Hotspots\")\n",
        "\n",
        "    # Define a dictionary of coordinates for major cities (Hardcoded for simplicity)\n",
        "    city_coords = {\n",
        "        'Delhi':(28.61,77.20), 'Mumbai':(19.07,72.87), 'Bengaluru':(12.97,77.59),\n",
        "        'Kolkata':(22.57, 88.36), 'Chennai':(13.08, 80.27), 'Hyderabad':(17.38, 78.48),\n",
        "        'Ahmedabad':(23.02, 72.57), 'Lucknow':(26.84, 80.94)\n",
        "    }\n",
        "\n",
        "    # Group data by City and calculate average AQI\n",
        "    city_aqi = df.groupby('City')['AQI'].mean().round(0).reset_index()\n",
        "\n",
        "    # Add Latitude and Longitude columns by mapping the city names to the dictionary\n",
        "    city_aqi['lat'] = city_aqi['City'].map({k:v[0] for k,v in city_coords.items()})\n",
        "    city_aqi['lon'] = city_aqi['City'].map({k:v[1] for k,v in city_coords.items()})\n",
        "\n",
        "    # Create a Scatter Mapbox plot\n",
        "    fig = px.scatter_mapbox(\n",
        "        city_aqi.dropna(),       # Remove any cities without coordinates\n",
        "        lat=\"lat\", lon=\"lon\",    # Set coordinates\n",
        "        size=\"AQI\", color=\"AQI\", # Dot size and color depend on AQI\n",
        "        hover_name=\"City\",       # Show city name on hover\n",
        "        zoom=3,                  # Initial zoom level\n",
        "        title=\"AQI Hotspots\",\n",
        "        color_continuous_scale=\"Reds\" # Red color scale for pollution\n",
        "    )\n",
        "    # Set the map style to a light theme\n",
        "    fig.update_layout(mapbox_style=\"carto-positron\", height=600)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# TAB 7: ABOUT\n",
        "with tab7:\n",
        "    st.header(\"About This Project\")\n",
        "    # Display project metadata using Markdown\n",
        "    st.markdown(\"\"\"\n",
        "    **CMP7005 ‚Äì Programming for Data Analysis** **Student ID:** ST20316895\n",
        "    **Academic Year:** 2025‚Äì26\n",
        "    **Module Leader:** aprasad@cardiffmet.ac.uk\n",
        "    **Dataset:** India Air Quality (2015‚Äì2020)\n",
        "    **Built with:** Python ‚Ä¢ Pandas ‚Ä¢ Scikit-learn ‚Ä¢ Streamlit ‚Ä¢ Plotly\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this code, I have a complete Streamlit application that loads and cleans the processed AQI dataset, presents dashboards and EDA views, shows seasonal patterns, trains and saves a Random Forest model, provides a live prediction interface with sliders, and maps city-level AQI hotspots. The underlying analysis code and models are still available in the notebook, but now I also have a user-friendly GUI that supports my pipeline and meets the \"From Data to Application Development\" requirement, ready to be run with `streamlit run` for demonstration and assessment."
      ],
      "metadata": {
        "id": "SJDmgsiGnSbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Save the Final File**\n",
        "I need to save the fully processed dataset to Google Drive so my entire pipeline is reproducible. This will allow my Streamlit app and later experiments to load a stable, final version of the data. Colab‚Äôs local filesystem is temporary. If I don‚Äôt export `df_scaled` to a Drive path, I risk losing all my cleaning, feature engineering, and scaling work when the session resets. Writing `India_Air_Quality_Final_Processed.csv` to a specific Drive location creates a single file that both the notebook and the app can reuse without having to run all preprocessing steps again. This approach also supports the assessment‚Äôs focus on good practice and version control. The exported file can be referenced in GitHub, backed up, and used consistently in different environments."
      ],
      "metadata": {
        "id": "7ruTTv7RCeCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the processed file in Google Drive\n",
        "output_drive_path = '/content/drive/MyDrive/India_Air_Quality_Final_Processed.csv'\n",
        "\n",
        "# Save the df_scaled DataFrame to the specified path in Google Drive\n",
        "df_scaled.to_csv(output_drive_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Successfully exported 'India_Air_Quality_Final_Processed.csv' to: {output_drive_path}\")"
      ],
      "metadata": {
        "id": "Zcvtx5vXZgyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Inferance:**\n",
        "After this step, `df_scaled` remains unchanged in memory. However, I now have a permanent CSV copy at `/content/drive/MyDrive/India_Air_Quality_Final_Processed.csv`. This file is ready to be loaded by the Streamlit app and by future notebooks. This ensures that my cleaned, transformed dataset is available even after Colab restarts and that the same processed data supports all upcoming modeling and GUI functionality."
      ],
      "metadata": {
        "id": "1MgcsMN4CmKN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

